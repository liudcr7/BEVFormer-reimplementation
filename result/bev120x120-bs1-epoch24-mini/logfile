2025-11-19 11:25:09,840 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1,2: NVIDIA A100-SXM4-40GB
CUDA_HOME: None
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.10.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.1+cu113
OpenCV: 4.12.0
MMCV: 1.6.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.28.2
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+
spconv2.0: False
------------------------------------------------------------

2025-11-19 11:25:09,842 - mmdet - INFO - Distributed training: False
2025-11-19 11:25:09,842 - mmdet - INFO - Config:
# BEVFormer configuration for nuScenes dataset
# This is a standalone config file without _base_ dependencies

# Point cloud range for nuScenes
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
voxel_size = [0.2, 0.2, 8]

# Image normalization
img_norm_cfg = dict(
    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)

# Class names for nuScenes (10 classes)
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]

input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)

# Model dimensions
_dim_ = 256
_pos_dim_ = _dim_ // 2
_ffn_dim_ = _dim_ * 2
_num_levels_ = 4
bev_h_ = 120
bev_w_ = 120
queue_length = 4  # each sequence contains `queue_length` frames

# Model configuration
model = dict(
    type='BEVFormer',
    use_grid_mask=True,
    enable_temporal_test=True,
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        # dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=True),  # Enable fallback for CUDA compatibility (sm_120)
        # stage_with_dcn=(False, False, True, True)
        ),
    img_neck=dict(
        type='FPN',
        in_channels=[512, 1024, 2048],
        out_channels=_dim_,
        start_level=0,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='BEVFormerHead',
        bev_h=bev_h_,
        bev_w=bev_w_,
        num_query=900,
        num_classes=10,
        in_channels=_dim_,
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        transformer=dict(
            type='PerceptionTransformer',
            rotate_prev_bev=True,
            use_shift=True,
            use_can_bus=True,
            embed_dims=_dim_,
            encoder=dict(
                type='BEVFormerEncoder',
                num_layers=6,
                pc_range=point_cloud_range,
                num_points_in_pillar=4,
                return_intermediate=False,
                transformerlayers=dict(
                    type='BEVFormerLayer',
                    attn_cfgs=[
                        dict(
                            type='TemporalSelfAttention',
                            embed_dims=_dim_,
                            num_levels=1, 
                            num_points=4,
                            num_heads=8,
                            num_bev_queue=2,
                            im2col_step=64,
                            dropout=0.1,
                            ),
                        dict(
                            type='SpatialCrossAttention',
                            pc_range=point_cloud_range,
                            deformable_attention=dict(
                                type='MSDeformableAttention3D',
                                embed_dims=_dim_,
                                num_levels=_num_levels_,
                                num_points=8,
                                num_heads=8,
                                im2col_step=64),
                            embed_dims=_dim_,
                        )
                    ],
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=_dim_,
                        feedforward_channels=_ffn_dim_,
                        num_fcs=2,
                        ffn_drop=0.1,
                        act_cfg=dict(type='ReLU', inplace=True)),
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm'))),
            decoder=dict(
                type='DetectionTransformerDecoder',
                num_layers=6,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=_dim_,
                            num_heads=8,
                            dropout=0.1),
                         dict(
                            type='CustomMSDeformableAttention',
                            embed_dims=_dim_,
                            num_levels=1,  
                            num_points=8),
                    ],
                    feedforward_channels=_ffn_dim_,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        bbox_coder=dict(
            type='NMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=point_cloud_range,
            max_num=300,
            voxel_size=voxel_size,
            num_classes=10),
        positional_encoding=dict(
            type='LearnedPositionalEncoding',
            num_feats=_pos_dim_,
            row_num_embed=bev_h_,
            col_num_embed=bev_w_,
            ),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        assigner=dict(
            type='HungarianAssigner3D',
            cls_cost=dict(type='FocalLossCost', weight=2.0, alpha=0.25, gamma=2.0),
            reg_cost=dict(type='BBox3DL1Cost', weight=0.25))),
    # model training and testing settings
    train_cfg=dict(pts=dict(
        grid_size=[512, 512, 1],
        voxel_size=voxel_size,
        point_cloud_range=point_cloud_range,
        out_size_factor=4)))

# Dataset configuration
dataset_type = 'CustomNuScenesDataset'
data_root = '/home/junjia/disk3/ld/v1.0-mini/nuscenes/'
file_client_args = dict(backend='disk')

# Training pipeline
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True, with_attr_label=False),
    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
    dict(type='ObjectNameFilter', classes=class_names),
    dict(type='NormalizeMultiviewImage', **img_norm_cfg),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(type='CustomDefaultFormatBundle3D', class_names=class_names),
    dict(type='CustomCollect3D', keys=['gt_bboxes_3d', 'gt_labels_3d', 'img'])
]

# Testing pipeline
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(type='NormalizeMultiviewImage', **img_norm_cfg),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='CustomDefaultFormatBundle3D',
                class_names=class_names,
                with_label=False),
            dict(type='CustomCollect3D', keys=['img'])
        ])
]

# Data configuration
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=8,  # Set to 0 on Windows to avoid multiprocessing pickle issues, though Windows may not work
    train=dict(
        type=dataset_type,
        data_root=data_root,
        ann_file=data_root + 'nuscenes_infos_temporal_train.pkl',
        pipeline=train_pipeline,
        classes=class_names,
        modality=input_modality,
        test_mode=False,
        use_valid_flag=True,
        bev_size=(bev_h_, bev_w_),
        queue_length=queue_length,
        box_type_3d='LiDAR'),
    val=dict(
        type=dataset_type,
             data_root=data_root,
             ann_file=data_root + 'nuscenes_infos_temporal_val.pkl',
        pipeline=test_pipeline,
        bev_size=(bev_h_, bev_w_),
        classes=class_names,
        modality=input_modality,
        samples_per_gpu=1),
    test=dict(
        type=dataset_type,
              data_root=data_root,
              ann_file=data_root + 'nuscenes_infos_temporal_val.pkl',
        pipeline=test_pipeline,
        bev_size=(bev_h_, bev_w_),
        classes=class_names,
        modality=input_modality),
    shuffler_sampler=dict(type='DistributedGroupSampler'),
    nonshuffler_sampler=dict(type='DistributedSampler'))

# Optimizer configuration
optimizer = dict(
    type='AdamW',
    lr=2e-4,
    paramwise_cfg=dict(
        custom_keys={
            'img_backbone': dict(lr_mult=0.1),
        }),
    weight_decay=0.01)

optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))

# Learning rate configuration
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=1.0 / 3,
    min_lr_ratio=1e-3)

total_epochs = 24

# Evaluation configuration
evaluation = dict(interval=1, pipeline=test_pipeline)

# Runner configuration
runner = dict(type='EpochBasedRunner', max_epochs=total_epochs)

# Load pretrained weights
load_from = 'ckpts/r101_dcn_fcos3d_pretrain.pth'

# Logging configuration
log_config = dict(
    interval=50,  # Log every 50 iterations
    hooks=[
        dict(
            type='TextLoggerHook',
            by_epoch=False,  # Log by iteration, not by epoch
            ignore_last=False,  # Log the last iteration
            reset_flag=False,  # Don't reset metrics after logging
            interval_exp_name=False,  # Don't include experiment name in log
        ),
        # TensorboardLoggerHook disabled due to setuptools compatibility issue
        # To enable: upgrade tensorboard: pip install --upgrade tensorboard
        # dict(type='TensorboardLoggerHook')
    ])

# Checkpoint configuration
checkpoint_config = dict(interval=1)

# Workflow configuration
# Format: [('train', 1)] for training only, [('train', 1), ('val', 1)] for train+val
workflow = [('train', 1)]

# Log level configuration
log_level = 'INFO'
2025-11-19 11:25:09,842 - mmdet - INFO - Set random seed to 0, deterministic: False
2025-11-19 11:25:09,842 - mmdet - INFO - Building training dataset...
2025-11-19 11:25:09,842 - mmdet - INFO - Dataset config: type=CustomNuScenesDataset, data_root=/home/junjia/disk3/ld/v1.0-mini/nuscenes/, ann_file=/home/junjia/disk3/ld/v1.0-mini/nuscenes/nuscenes_infos_temporal_train.pkl
2025-11-19 11:25:09,842 - mmdet - INFO - Annotation file exists: /home/junjia/disk3/ld/v1.0-mini/nuscenes/nuscenes_infos_temporal_train.pkl, size: 6.47 MB
2025-11-19 11:25:09,843 - mmdet - INFO - Starting dataset initialization (this may take a while for large datasets)...
2025-11-19 11:25:09,896 - mmdet - INFO - Dataset built successfully. Dataset length: 323
2025-11-19 11:25:09,896 - mmdet - INFO - Checking workflow configuration...
2025-11-19 11:25:09,896 - mmdet - INFO - Workflow: [('train', 1)], length: 1
2025-11-19 11:25:09,897 - mmdet - INFO - Building model...
2025-11-19 11:25:09,897 - mmdet - INFO - Model config type: BEVFormer
2025-11-19 11:25:09,897 - mmdet - INFO - Model config keys: ['type', 'use_grid_mask', 'enable_temporal_test', 'img_backbone', 'img_neck', 'pts_bbox_head', 'train_cfg']...
2025-11-19 11:25:09,897 - mmdet - INFO - Calling build_model function...
2025-11-19 11:25:09,897 - mmdet - INFO - train_cfg: None
2025-11-19 11:25:09,897 - mmdet - INFO - test_cfg: None
2025-11-19 11:25:09,897 - mmdet - INFO - About to call build_model, stack trace:
2025-11-19 11:25:09,897 - mmdet - INFO -   File "tools/train.py", line 443, in <module>
    main()

2025-11-19 11:25:09,897 - mmdet - INFO - CUDA available: True
2025-11-19 11:25:09,897 - mmdet - INFO - CUDA device count: 3
2025-11-19 11:25:09,897 - mmdet - INFO - Current CUDA device: 0
2025-11-19 11:25:09,897 - mmdet - INFO - CUDA device name: NVIDIA A100-SXM4-40GB
2025-11-19 11:25:09,897 - mmdet - INFO - Entering build_model...
2025-11-19 11:25:09,897 - mmdet - INFO - This may take a while - building ResNet101, FPN, and BEVFormerHead...
2025-11-19 11:25:09,897 - mmdet - INFO - If this hangs, it might be building img_backbone (ResNet101) or img_neck (FPN)...
2025-11-19 11:25:09,897 - mmdet - INFO - Note: ResNet101 with DCNv2 can take 30-60 seconds to build on first run
2025-11-19 11:25:10,403 - mmdet - INFO - build_model returned successfully (took 0.50 seconds)
2025-11-19 11:25:10,403 - mmdet - INFO - Model built. Initializing weights...
2025-11-19 11:25:10,523 - mmdet - INFO - initialize LearnedPositionalEncoding with init_cfg {'type': 'Uniform', 'layer': 'Embedding'}
2025-11-19 11:25:10,537 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-11-19 11:25:10,869 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,870 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,870 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,871 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,872 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,873 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,874 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,875 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,876 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,877 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,878 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,878 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,879 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,880 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,881 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,881 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,882 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,883 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,884 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,884 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,885 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,886 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,886 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,887 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,888 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,889 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,890 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,890 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,891 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,892 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,898 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,900 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,901 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-11-19 11:25:10,911 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_bbox_head.code_weights - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.cams_embeds - torch.Size([6, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight - torch.Size([64, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.encoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([64, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([512, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.reference_points.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.reference_points.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.can_bus_mlp.0.weight - torch.Size([128, 18]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.can_bus_mlp.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.can_bus_mlp.2.weight - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in PerceptionTransformer  

pts_bbox_head.transformer.can_bus_mlp.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.can_bus_mlp.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.transformer.can_bus_mlp.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.positional_encoding.row_embed.weight - torch.Size([120, 128]): 
UniformInit: a=0.0, b=1.0, bias=0 

pts_bbox_head.positional_encoding.col_embed.weight - torch.Size([120, 128]): 
UniformInit: a=0.0, b=1.0, bias=0 

pts_bbox_head.cls_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.0.6.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.1.6.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.2.6.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.3.6.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.4.6.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.3.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.6.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.cls_branches.5.6.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.bev_embedding.weight - torch.Size([14400, 256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

pts_bbox_head.query_embedding.weight - torch.Size([900, 512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of BEVFormer  
2025-11-19 11:25:10,944 - mmdet - INFO - Model weights initialized.
2025-11-19 11:25:10,947 - mmdet - INFO - Model:
BEVFormer(
  (pts_bbox_head): BEVFormerHead(
    (transformer): PerceptionTransformer(
      (encoder): BEVFormerEncoder(
        (layers): ModuleList(
          (0): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): BEVFormerLayer(
            (attentions): ModuleList(
              (0): TemporalSelfAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=512, out_features=128, bias=True)
                (attention_weights): Linear(in_features=512, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (1): SpatialCrossAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (deformable_attention): MSDeformableAttention3D(
                  (sampling_offsets): Linear(in_features=256, out_features=512, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=256, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (decoder): DetectionTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
                (attention_weights): Linear(in_features=256, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
                (attention_weights): Linear(in_features=256, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
                (attention_weights): Linear(in_features=256, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
                (attention_weights): Linear(in_features=256, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
                (attention_weights): Linear(in_features=256, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): CustomMSDeformableAttention(
                (sampling_offsets): Linear(in_features=256, out_features=128, bias=True)
                (attention_weights): Linear(in_features=256, out_features=64, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=512, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=512, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (reference_points): Linear(in_features=256, out_features=3, bias=True)
      (can_bus_mlp): Sequential(
        (0): Linear(in_features=18, out_features=128, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=128, out_features=256, bias=True)
        (3): ReLU(inplace=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (positional_encoding): LearnedPositionalEncoding(num_feats=128, row_num_embed=120, col_num_embed=120)
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (bev_embedding): Embedding(14400, 256)
    (query_embedding): Embedding(900, 512)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (grid_mask): GridMask()
)
2025-11-19 11:25:14,903 - mmdet - INFO - [Heartbeat] Still building model... (5s elapsed)
2025-11-19 11:25:15,245 - mmdet - INFO - load checkpoint from local path: ckpts/r101_dcn_fcos3d_pretrain.pth
2025-11-19 11:25:15,502 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: bbox_head.bld_alpha, bbox_head.cls_convs.0.conv.weight, bbox_head.cls_convs.0.conv.bias, bbox_head.cls_convs.0.gn.weight, bbox_head.cls_convs.0.gn.bias, bbox_head.cls_convs.1.conv.weight, bbox_head.cls_convs.1.conv.bias, bbox_head.cls_convs.1.conv.conv_offset.weight, bbox_head.cls_convs.1.conv.conv_offset.bias, bbox_head.cls_convs.1.gn.weight, bbox_head.cls_convs.1.gn.bias, bbox_head.reg_convs.0.conv.weight, bbox_head.reg_convs.0.conv.bias, bbox_head.reg_convs.0.gn.weight, bbox_head.reg_convs.0.gn.bias, bbox_head.reg_convs.1.conv.weight, bbox_head.reg_convs.1.conv.bias, bbox_head.reg_convs.1.conv.conv_offset.weight, bbox_head.reg_convs.1.conv.conv_offset.bias, bbox_head.reg_convs.1.gn.weight, bbox_head.reg_convs.1.gn.bias, bbox_head.conv_cls_prev.0.conv.weight, bbox_head.conv_cls_prev.0.conv.bias, bbox_head.conv_cls_prev.0.gn.weight, bbox_head.conv_cls_prev.0.gn.bias, bbox_head.conv_cls.weight, bbox_head.conv_cls.bias, bbox_head.conv_reg_prevs.0.0.conv.weight, bbox_head.conv_reg_prevs.0.0.conv.bias, bbox_head.conv_reg_prevs.0.0.gn.weight, bbox_head.conv_reg_prevs.0.0.gn.bias, bbox_head.conv_reg_prevs.1.0.conv.weight, bbox_head.conv_reg_prevs.1.0.conv.bias, bbox_head.conv_reg_prevs.1.0.gn.weight, bbox_head.conv_reg_prevs.1.0.gn.bias, bbox_head.conv_reg_prevs.2.0.conv.weight, bbox_head.conv_reg_prevs.2.0.conv.bias, bbox_head.conv_reg_prevs.2.0.gn.weight, bbox_head.conv_reg_prevs.2.0.gn.bias, bbox_head.conv_reg_prevs.3.0.conv.weight, bbox_head.conv_reg_prevs.3.0.conv.bias, bbox_head.conv_reg_prevs.3.0.gn.weight, bbox_head.conv_reg_prevs.3.0.gn.bias, bbox_head.conv_reg_prevs.5.0.conv.weight, bbox_head.conv_reg_prevs.5.0.conv.bias, bbox_head.conv_reg_prevs.5.0.gn.weight, bbox_head.conv_reg_prevs.5.0.gn.bias, bbox_head.conv_regs.0.weight, bbox_head.conv_regs.0.bias, bbox_head.conv_regs.1.weight, bbox_head.conv_regs.1.bias, bbox_head.conv_regs.2.weight, bbox_head.conv_regs.2.bias, bbox_head.conv_regs.3.weight, bbox_head.conv_regs.3.bias, bbox_head.conv_regs.4.weight, bbox_head.conv_regs.4.bias, bbox_head.conv_regs.5.weight, bbox_head.conv_regs.5.bias, bbox_head.conv_dir_cls_prev.0.conv.weight, bbox_head.conv_dir_cls_prev.0.conv.bias, bbox_head.conv_dir_cls_prev.0.gn.weight, bbox_head.conv_dir_cls_prev.0.gn.bias, bbox_head.conv_dir_cls.weight, bbox_head.conv_dir_cls.bias, bbox_head.conv_attr_prev.0.conv.weight, bbox_head.conv_attr_prev.0.conv.bias, bbox_head.conv_attr_prev.0.gn.weight, bbox_head.conv_attr_prev.0.gn.bias, bbox_head.conv_attr.weight, bbox_head.conv_attr.bias, bbox_head.conv_depth_cls_prev.0.conv.weight, bbox_head.conv_depth_cls_prev.0.conv.bias, bbox_head.conv_depth_cls_prev.0.gn.weight, bbox_head.conv_depth_cls_prev.0.gn.bias, bbox_head.conv_depth_cls.weight, bbox_head.conv_depth_cls.bias, bbox_head.conv_centerness_prev.0.conv.weight, bbox_head.conv_centerness_prev.0.conv.bias, bbox_head.conv_centerness_prev.0.gn.weight, bbox_head.conv_centerness_prev.0.gn.bias, bbox_head.conv_centerness.weight, bbox_head.conv_centerness.bias, bbox_head.scales.0.0.scale, bbox_head.scales.0.1.scale, bbox_head.scales.0.2.scale, bbox_head.scales.0.3.scale, bbox_head.scales.1.0.scale, bbox_head.scales.1.1.scale, bbox_head.scales.1.2.scale, bbox_head.scales.1.3.scale, bbox_head.scales.2.0.scale, bbox_head.scales.2.1.scale, bbox_head.scales.2.2.scale, bbox_head.scales.2.3.scale, bbox_head.scales.3.0.scale, bbox_head.scales.3.1.scale, bbox_head.scales.3.2.scale, bbox_head.scales.3.3.scale, bbox_head.scales.4.0.scale, bbox_head.scales.4.1.scale, bbox_head.scales.4.2.scale, bbox_head.scales.4.3.scale, img_backbone.layer3.0.conv2.conv_offset.weight, img_backbone.layer3.0.conv2.conv_offset.bias, img_backbone.layer3.1.conv2.conv_offset.weight, img_backbone.layer3.1.conv2.conv_offset.bias, img_backbone.layer3.2.conv2.conv_offset.weight, img_backbone.layer3.2.conv2.conv_offset.bias, img_backbone.layer3.3.conv2.conv_offset.weight, img_backbone.layer3.3.conv2.conv_offset.bias, img_backbone.layer3.4.conv2.conv_offset.weight, img_backbone.layer3.4.conv2.conv_offset.bias, img_backbone.layer3.5.conv2.conv_offset.weight, img_backbone.layer3.5.conv2.conv_offset.bias, img_backbone.layer3.6.conv2.conv_offset.weight, img_backbone.layer3.6.conv2.conv_offset.bias, img_backbone.layer3.7.conv2.conv_offset.weight, img_backbone.layer3.7.conv2.conv_offset.bias, img_backbone.layer3.8.conv2.conv_offset.weight, img_backbone.layer3.8.conv2.conv_offset.bias, img_backbone.layer3.9.conv2.conv_offset.weight, img_backbone.layer3.9.conv2.conv_offset.bias, img_backbone.layer3.10.conv2.conv_offset.weight, img_backbone.layer3.10.conv2.conv_offset.bias, img_backbone.layer3.11.conv2.conv_offset.weight, img_backbone.layer3.11.conv2.conv_offset.bias, img_backbone.layer3.12.conv2.conv_offset.weight, img_backbone.layer3.12.conv2.conv_offset.bias, img_backbone.layer3.13.conv2.conv_offset.weight, img_backbone.layer3.13.conv2.conv_offset.bias, img_backbone.layer3.14.conv2.conv_offset.weight, img_backbone.layer3.14.conv2.conv_offset.bias, img_backbone.layer3.15.conv2.conv_offset.weight, img_backbone.layer3.15.conv2.conv_offset.bias, img_backbone.layer3.16.conv2.conv_offset.weight, img_backbone.layer3.16.conv2.conv_offset.bias, img_backbone.layer3.17.conv2.conv_offset.weight, img_backbone.layer3.17.conv2.conv_offset.bias, img_backbone.layer3.18.conv2.conv_offset.weight, img_backbone.layer3.18.conv2.conv_offset.bias, img_backbone.layer3.19.conv2.conv_offset.weight, img_backbone.layer3.19.conv2.conv_offset.bias, img_backbone.layer3.20.conv2.conv_offset.weight, img_backbone.layer3.20.conv2.conv_offset.bias, img_backbone.layer3.21.conv2.conv_offset.weight, img_backbone.layer3.21.conv2.conv_offset.bias, img_backbone.layer3.22.conv2.conv_offset.weight, img_backbone.layer3.22.conv2.conv_offset.bias, img_backbone.layer4.0.conv2.conv_offset.weight, img_backbone.layer4.0.conv2.conv_offset.bias, img_backbone.layer4.1.conv2.conv_offset.weight, img_backbone.layer4.1.conv2.conv_offset.bias, img_backbone.layer4.2.conv2.conv_offset.weight, img_backbone.layer4.2.conv2.conv_offset.bias, img_neck.fpn_convs.4.conv.weight, img_neck.fpn_convs.4.conv.bias

missing keys in source state_dict: pts_bbox_head.code_weights, pts_bbox_head.transformer.level_embeds, pts_bbox_head.transformer.cams_embeds, pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.0.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.0.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.0.value_proj.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.0.output_proj.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.deformable_attention.value_proj.bias, pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.weight, pts_bbox_head.transformer.encoder.layers.0.attentions.1.output_proj.bias, pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.weight, pts_bbox_head.transformer.encoder.layers.0.ffns.0.layers.1.bias, pts_bbox_head.transformer.encoder.layers.0.norms.0.weight, pts_bbox_head.transformer.encoder.layers.0.norms.0.bias, pts_bbox_head.transformer.encoder.layers.0.norms.1.weight, pts_bbox_head.transformer.encoder.layers.0.norms.1.bias, pts_bbox_head.transformer.encoder.layers.0.norms.2.weight, pts_bbox_head.transformer.encoder.layers.0.norms.2.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.0.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.0.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.0.value_proj.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.0.output_proj.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.deformable_attention.value_proj.bias, pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.weight, pts_bbox_head.transformer.encoder.layers.1.attentions.1.output_proj.bias, pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.weight, pts_bbox_head.transformer.encoder.layers.1.ffns.0.layers.1.bias, pts_bbox_head.transformer.encoder.layers.1.norms.0.weight, pts_bbox_head.transformer.encoder.layers.1.norms.0.bias, pts_bbox_head.transformer.encoder.layers.1.norms.1.weight, pts_bbox_head.transformer.encoder.layers.1.norms.1.bias, pts_bbox_head.transformer.encoder.layers.1.norms.2.weight, pts_bbox_head.transformer.encoder.layers.1.norms.2.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.0.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.0.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.0.value_proj.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.0.output_proj.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.deformable_attention.value_proj.bias, pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.weight, pts_bbox_head.transformer.encoder.layers.2.attentions.1.output_proj.bias, pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.weight, pts_bbox_head.transformer.encoder.layers.2.ffns.0.layers.1.bias, pts_bbox_head.transformer.encoder.layers.2.norms.0.weight, pts_bbox_head.transformer.encoder.layers.2.norms.0.bias, pts_bbox_head.transformer.encoder.layers.2.norms.1.weight, pts_bbox_head.transformer.encoder.layers.2.norms.1.bias, pts_bbox_head.transformer.encoder.layers.2.norms.2.weight, pts_bbox_head.transformer.encoder.layers.2.norms.2.bias, pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.3.attentions.0.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.3.attentions.0.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.weight, pts_bbox_head.transformer.encoder.layers.3.attentions.0.value_proj.bias, pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.weight, pts_bbox_head.transformer.encoder.layers.3.attentions.0.output_proj.bias, pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.3.attentions.1.deformable_attention.value_proj.bias, pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.weight, pts_bbox_head.transformer.encoder.layers.3.attentions.1.output_proj.bias, pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.weight, pts_bbox_head.transformer.encoder.layers.3.ffns.0.layers.1.bias, pts_bbox_head.transformer.encoder.layers.3.norms.0.weight, pts_bbox_head.transformer.encoder.layers.3.norms.0.bias, pts_bbox_head.transformer.encoder.layers.3.norms.1.weight, pts_bbox_head.transformer.encoder.layers.3.norms.1.bias, pts_bbox_head.transformer.encoder.layers.3.norms.2.weight, pts_bbox_head.transformer.encoder.layers.3.norms.2.bias, pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.4.attentions.0.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.4.attentions.0.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.weight, pts_bbox_head.transformer.encoder.layers.4.attentions.0.value_proj.bias, pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.weight, pts_bbox_head.transformer.encoder.layers.4.attentions.0.output_proj.bias, pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.4.attentions.1.deformable_attention.value_proj.bias, pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.weight, pts_bbox_head.transformer.encoder.layers.4.attentions.1.output_proj.bias, pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.weight, pts_bbox_head.transformer.encoder.layers.4.ffns.0.layers.1.bias, pts_bbox_head.transformer.encoder.layers.4.norms.0.weight, pts_bbox_head.transformer.encoder.layers.4.norms.0.bias, pts_bbox_head.transformer.encoder.layers.4.norms.1.weight, pts_bbox_head.transformer.encoder.layers.4.norms.1.bias, pts_bbox_head.transformer.encoder.layers.4.norms.2.weight, pts_bbox_head.transformer.encoder.layers.4.norms.2.bias, pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.5.attentions.0.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.5.attentions.0.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.weight, pts_bbox_head.transformer.encoder.layers.5.attentions.0.value_proj.bias, pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.weight, pts_bbox_head.transformer.encoder.layers.5.attentions.0.output_proj.bias, pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.weight, pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.sampling_offsets.bias, pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.weight, pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.attention_weights.bias, pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.weight, pts_bbox_head.transformer.encoder.layers.5.attentions.1.deformable_attention.value_proj.bias, pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.weight, pts_bbox_head.transformer.encoder.layers.5.attentions.1.output_proj.bias, pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.weight, pts_bbox_head.transformer.encoder.layers.5.ffns.0.layers.1.bias, pts_bbox_head.transformer.encoder.layers.5.norms.0.weight, pts_bbox_head.transformer.encoder.layers.5.norms.0.bias, pts_bbox_head.transformer.encoder.layers.5.norms.1.weight, pts_bbox_head.transformer.encoder.layers.5.norms.1.bias, pts_bbox_head.transformer.encoder.layers.5.norms.2.weight, pts_bbox_head.transformer.encoder.layers.5.norms.2.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias, pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight, pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias, pts_bbox_head.transformer.decoder.layers.0.norms.0.weight, pts_bbox_head.transformer.decoder.layers.0.norms.0.bias, pts_bbox_head.transformer.decoder.layers.0.norms.1.weight, pts_bbox_head.transformer.decoder.layers.0.norms.1.bias, pts_bbox_head.transformer.decoder.layers.0.norms.2.weight, pts_bbox_head.transformer.decoder.layers.0.norms.2.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias, pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight, pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias, pts_bbox_head.transformer.decoder.layers.1.norms.0.weight, pts_bbox_head.transformer.decoder.layers.1.norms.0.bias, pts_bbox_head.transformer.decoder.layers.1.norms.1.weight, pts_bbox_head.transformer.decoder.layers.1.norms.1.bias, pts_bbox_head.transformer.decoder.layers.1.norms.2.weight, pts_bbox_head.transformer.decoder.layers.1.norms.2.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias, pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight, pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias, pts_bbox_head.transformer.decoder.layers.2.norms.0.weight, pts_bbox_head.transformer.decoder.layers.2.norms.0.bias, pts_bbox_head.transformer.decoder.layers.2.norms.1.weight, pts_bbox_head.transformer.decoder.layers.2.norms.1.bias, pts_bbox_head.transformer.decoder.layers.2.norms.2.weight, pts_bbox_head.transformer.decoder.layers.2.norms.2.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias, pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight, pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias, pts_bbox_head.transformer.decoder.layers.3.norms.0.weight, pts_bbox_head.transformer.decoder.layers.3.norms.0.bias, pts_bbox_head.transformer.decoder.layers.3.norms.1.weight, pts_bbox_head.transformer.decoder.layers.3.norms.1.bias, pts_bbox_head.transformer.decoder.layers.3.norms.2.weight, pts_bbox_head.transformer.decoder.layers.3.norms.2.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias, pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight, pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias, pts_bbox_head.transformer.decoder.layers.4.norms.0.weight, pts_bbox_head.transformer.decoder.layers.4.norms.0.bias, pts_bbox_head.transformer.decoder.layers.4.norms.1.weight, pts_bbox_head.transformer.decoder.layers.4.norms.1.bias, pts_bbox_head.transformer.decoder.layers.4.norms.2.weight, pts_bbox_head.transformer.decoder.layers.4.norms.2.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias, pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight, pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias, pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight, pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias, pts_bbox_head.transformer.decoder.layers.5.norms.0.weight, pts_bbox_head.transformer.decoder.layers.5.norms.0.bias, pts_bbox_head.transformer.decoder.layers.5.norms.1.weight, pts_bbox_head.transformer.decoder.layers.5.norms.1.bias, pts_bbox_head.transformer.decoder.layers.5.norms.2.weight, pts_bbox_head.transformer.decoder.layers.5.norms.2.bias, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias, pts_bbox_head.transformer.can_bus_mlp.0.weight, pts_bbox_head.transformer.can_bus_mlp.0.bias, pts_bbox_head.transformer.can_bus_mlp.2.weight, pts_bbox_head.transformer.can_bus_mlp.2.bias, pts_bbox_head.transformer.can_bus_mlp.4.weight, pts_bbox_head.transformer.can_bus_mlp.4.bias, pts_bbox_head.positional_encoding.row_embed.weight, pts_bbox_head.positional_encoding.col_embed.weight, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias, pts_bbox_head.reg_branches.0.0.weight, pts_bbox_head.reg_branches.0.0.bias, pts_bbox_head.reg_branches.0.2.weight, pts_bbox_head.reg_branches.0.2.bias, pts_bbox_head.reg_branches.0.4.weight, pts_bbox_head.reg_branches.0.4.bias, pts_bbox_head.reg_branches.1.0.weight, pts_bbox_head.reg_branches.1.0.bias, pts_bbox_head.reg_branches.1.2.weight, pts_bbox_head.reg_branches.1.2.bias, pts_bbox_head.reg_branches.1.4.weight, pts_bbox_head.reg_branches.1.4.bias, pts_bbox_head.reg_branches.2.0.weight, pts_bbox_head.reg_branches.2.0.bias, pts_bbox_head.reg_branches.2.2.weight, pts_bbox_head.reg_branches.2.2.bias, pts_bbox_head.reg_branches.2.4.weight, pts_bbox_head.reg_branches.2.4.bias, pts_bbox_head.reg_branches.3.0.weight, pts_bbox_head.reg_branches.3.0.bias, pts_bbox_head.reg_branches.3.2.weight, pts_bbox_head.reg_branches.3.2.bias, pts_bbox_head.reg_branches.3.4.weight, pts_bbox_head.reg_branches.3.4.bias, pts_bbox_head.reg_branches.4.0.weight, pts_bbox_head.reg_branches.4.0.bias, pts_bbox_head.reg_branches.4.2.weight, pts_bbox_head.reg_branches.4.2.bias, pts_bbox_head.reg_branches.4.4.weight, pts_bbox_head.reg_branches.4.4.bias, pts_bbox_head.reg_branches.5.0.weight, pts_bbox_head.reg_branches.5.0.bias, pts_bbox_head.reg_branches.5.2.weight, pts_bbox_head.reg_branches.5.2.bias, pts_bbox_head.reg_branches.5.4.weight, pts_bbox_head.reg_branches.5.4.bias, pts_bbox_head.bev_embedding.weight, pts_bbox_head.query_embedding.weight

2025-11-19 11:25:15,507 - mmdet - INFO - Start running, host: junjia@poweredgexe8545, work_dir: /mnt/disk3/ld/BEVFORMER/work_dirs/config120
2025-11-19 11:25:15,507 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2025-11-19 11:25:15,508 - mmdet - INFO - workflow: [('train', 1)], max: 24 epochs
2025-11-19 11:25:15,508 - mmdet - INFO - Checkpoints will be saved to /mnt/disk3/ld/BEVFORMER/work_dirs/config120 by HardDiskBackend.
2025-11-19 11:26:26,824 - mmdet - INFO - Iter [50/7752]	lr: 7.973e-05, eta: 3:03:04, time: 1.426, data_time: 0.191, memory: 29586, d0.loss_cls: 0.4081, d0.loss_bbox: 1.8713, d1.loss_cls: 0.3983, d1.loss_bbox: 1.8599, d2.loss_cls: 0.3923, d2.loss_bbox: 1.8667, d3.loss_cls: 0.3930, d3.loss_bbox: 1.8566, d4.loss_cls: 0.3937, d4.loss_bbox: 1.8857, loss_cls: 0.3937, loss_bbox: 1.8708, loss: 13.5903, grad_norm: 49.4027
2025-11-19 11:27:29,792 - mmdet - INFO - Iter [100/7752]	lr: 9.307e-05, eta: 2:51:15, time: 1.259, data_time: 0.051, memory: 29604, d0.loss_cls: 0.3292, d0.loss_bbox: 1.7647, d1.loss_cls: 0.3249, d1.loss_bbox: 1.7429, d2.loss_cls: 0.3256, d2.loss_bbox: 1.7624, d3.loss_cls: 0.3257, d3.loss_bbox: 1.7644, d4.loss_cls: 0.3292, d4.loss_bbox: 1.7702, loss_cls: 0.3274, loss_bbox: 1.7647, loss: 12.5312, grad_norm: 39.3690
2025-11-19 11:28:33,054 - mmdet - INFO - Iter [150/7752]	lr: 1.064e-04, eta: 2:46:51, time: 1.265, data_time: 0.056, memory: 29604, d0.loss_cls: 0.2900, d0.loss_bbox: 1.7353, d1.loss_cls: 0.2900, d1.loss_bbox: 1.7598, d2.loss_cls: 0.2931, d2.loss_bbox: 1.7193, d3.loss_cls: 0.2929, d3.loss_bbox: 1.7000, d4.loss_cls: 0.2943, d4.loss_bbox: 1.7149, loss_cls: 0.2939, loss_bbox: 1.7593, loss: 12.1427, grad_norm: 39.5112
2025-11-19 11:29:36,652 - mmdet - INFO - Iter [200/7752]	lr: 1.197e-04, eta: 2:44:20, time: 1.272, data_time: 0.057, memory: 29729, d0.loss_cls: 0.2655, d0.loss_bbox: 1.7762, d1.loss_cls: 0.2602, d1.loss_bbox: 1.7922, d2.loss_cls: 0.2696, d2.loss_bbox: 1.7537, d3.loss_cls: 0.2713, d3.loss_bbox: 1.7506, d4.loss_cls: 0.2683, d4.loss_bbox: 1.8034, loss_cls: 0.2705, loss_bbox: 1.7840, loss: 12.2654, grad_norm: 44.7253
2025-11-19 11:30:39,772 - mmdet - INFO - Iter [250/7752]	lr: 1.331e-04, eta: 2:42:10, time: 1.262, data_time: 0.055, memory: 29766, d0.loss_cls: 0.2548, d0.loss_bbox: 1.7319, d1.loss_cls: 0.2509, d1.loss_bbox: 1.7627, d2.loss_cls: 0.2629, d2.loss_bbox: 1.6896, d3.loss_cls: 0.2616, d3.loss_bbox: 1.7565, d4.loss_cls: 0.2645, d4.loss_bbox: 1.7823, loss_cls: 0.2656, loss_bbox: 1.7947, loss: 12.0779, grad_norm: 43.8300
2025-11-19 11:31:42,814 - mmdet - INFO - Iter [300/7752]	lr: 1.464e-04, eta: 2:40:20, time: 1.261, data_time: 0.054, memory: 29766, d0.loss_cls: 0.2783, d0.loss_bbox: 1.7147, d1.loss_cls: 0.2787, d1.loss_bbox: 1.7257, d2.loss_cls: 0.2842, d2.loss_bbox: 1.7333, d3.loss_cls: 0.2820, d3.loss_bbox: 1.7316, d4.loss_cls: 0.2845, d4.loss_bbox: 1.7855, loss_cls: 0.2858, loss_bbox: 1.7949, loss: 12.1794, grad_norm: 42.1127
2025-11-19 11:32:11,773 - mmdet - INFO - Iter [323/7752]	lr: 1.525e-04, eta: 2:52:36, time: 1.260, data_time: 0.053, memory: 29766, d0.loss_cls: 0.2629, d0.loss_bbox: 1.7121, d1.loss_cls: 0.2664, d1.loss_bbox: 1.7147, d2.loss_cls: 0.2665, d2.loss_bbox: 1.7464, d3.loss_cls: 0.2624, d3.loss_bbox: 1.7985, d4.loss_cls: 0.2634, d4.loss_bbox: 1.7950, loss_cls: 0.2644, loss_bbox: 1.7708, loss: 12.1235, grad_norm: 43.8309
2025-11-19 11:32:11,774 - mmdet - INFO - Saving checkpoint at 1 epochs
2025-11-19 11:32:51,676 - mmdet - INFO - Iter [350/7752]	lr: 1.591e-04, eta: 3:03:59, time: 1.434, data_time: 0.225, memory: 29766, d0.loss_cls: 0.3113, d0.loss_bbox: 1.6900, d1.loss_cls: 0.3124, d1.loss_bbox: 1.6686, d2.loss_cls: 0.3172, d2.loss_bbox: 1.7292, d3.loss_cls: 0.3117, d3.loss_bbox: 1.7898, d4.loss_cls: 0.3134, d4.loss_bbox: 1.7756, loss_cls: 0.3172, loss_bbox: 1.7951, loss: 12.3316, grad_norm: 39.0003
2025-11-19 11:33:54,422 - mmdet - INFO - Iter [400/7752]	lr: 1.723e-04, eta: 2:59:07, time: 1.255, data_time: 0.052, memory: 29766, d0.loss_cls: 0.2615, d0.loss_bbox: 1.6832, d1.loss_cls: 0.2568, d1.loss_bbox: 1.7495, d2.loss_cls: 0.2661, d2.loss_bbox: 1.7016, d3.loss_cls: 0.2651, d3.loss_bbox: 1.7026, d4.loss_cls: 0.2677, d4.loss_bbox: 1.6927, loss_cls: 0.2691, loss_bbox: 1.6991, loss: 11.8150, grad_norm: 41.8605
2025-11-19 11:34:57,307 - mmdet - INFO - Iter [450/7752]	lr: 1.856e-04, eta: 2:55:08, time: 1.258, data_time: 0.054, memory: 29766, d0.loss_cls: 0.2586, d0.loss_bbox: 1.7020, d1.loss_cls: 0.2656, d1.loss_bbox: 1.7118, d2.loss_cls: 0.2677, d2.loss_bbox: 1.7152, d3.loss_cls: 0.2650, d3.loss_bbox: 1.7150, d4.loss_cls: 0.2691, d4.loss_bbox: 1.7488, loss_cls: 0.2672, loss_bbox: 1.7819, loss: 11.9681, grad_norm: 43.3300
2025-11-19 11:36:00,317 - mmdet - INFO - Iter [500/7752]	lr: 1.989e-04, eta: 2:51:47, time: 1.260, data_time: 0.054, memory: 29766, d0.loss_cls: 0.2633, d0.loss_bbox: 1.6706, d1.loss_cls: 0.2693, d1.loss_bbox: 1.6587, d2.loss_cls: 0.2801, d2.loss_bbox: 1.6946, d3.loss_cls: 0.2742, d3.loss_bbox: 1.6600, d4.loss_cls: 0.2781, d4.loss_bbox: 1.6805, loss_cls: 0.2803, loss_bbox: 1.6835, loss: 11.6933, grad_norm: 38.8405
2025-11-19 11:37:03,019 - mmdet - INFO - Iter [550/7752]	lr: 1.991e-04, eta: 2:48:46, time: 1.254, data_time: 0.047, memory: 29766, d0.loss_cls: 0.2494, d0.loss_bbox: 1.6752, d1.loss_cls: 0.2562, d1.loss_bbox: 1.6902, d2.loss_cls: 0.2662, d2.loss_bbox: 1.6434, d3.loss_cls: 0.2625, d3.loss_bbox: 1.6504, d4.loss_cls: 0.2626, d4.loss_bbox: 1.6694, loss_cls: 0.2630, loss_bbox: 1.6526, loss: 11.5412, grad_norm: 38.4800
2025-11-19 11:38:06,014 - mmdet - INFO - Iter [600/7752]	lr: 1.991e-04, eta: 2:46:09, time: 1.260, data_time: 0.053, memory: 29766, d0.loss_cls: 0.2587, d0.loss_bbox: 1.6014, d1.loss_cls: 0.2696, d1.loss_bbox: 1.6292, d2.loss_cls: 0.2775, d2.loss_bbox: 1.6628, d3.loss_cls: 0.2739, d3.loss_bbox: 1.6557, d4.loss_cls: 0.2793, d4.loss_bbox: 1.7280, loss_cls: 0.2717, loss_bbox: 1.6868, loss: 11.5945, grad_norm: 39.8252
2025-11-19 11:39:03,617 - mmdet - INFO - Iter [646/7752]	lr: 1.991e-04, eta: 2:44:50, time: 1.255, data_time: 0.051, memory: 29766, d0.loss_cls: 0.2683, d0.loss_bbox: 1.5946, d1.loss_cls: 0.2812, d1.loss_bbox: 1.5902, d2.loss_cls: 0.2879, d2.loss_bbox: 1.6435, d3.loss_cls: 0.2857, d3.loss_bbox: 1.6429, d4.loss_cls: 0.2902, d4.loss_bbox: 1.6997, loss_cls: 0.2844, loss_bbox: 1.7403, loss: 11.6088, grad_norm: 40.1130
2025-11-19 11:39:03,617 - mmdet - INFO - Saving checkpoint at 2 epochs
2025-11-19 11:39:14,537 - mmdet - INFO - Iter [650/7752]	lr: 1.966e-04, eta: 3:05:56, time: 2.440, data_time: 1.212, memory: 29766, d0.loss_cls: 0.3420, d0.loss_bbox: 1.6094, d1.loss_cls: 0.3685, d1.loss_bbox: 1.6756, d2.loss_cls: 0.3716, d2.loss_bbox: 1.6991, d3.loss_cls: 0.3635, d3.loss_bbox: 1.7632, d4.loss_cls: 0.3745, d4.loss_bbox: 2.5712, loss_cls: 0.3555, loss_bbox: 2.7443, loss: 14.2385, grad_norm: 50.2361
2025-11-19 11:40:17,122 - mmdet - INFO - Iter [700/7752]	lr: 1.966e-04, eta: 3:01:57, time: 1.252, data_time: 0.050, memory: 29766, d0.loss_cls: 0.2425, d0.loss_bbox: 1.5507, d1.loss_cls: 0.2500, d1.loss_bbox: 1.5629, d2.loss_cls: 0.2593, d2.loss_bbox: 1.5848, d3.loss_cls: 0.2561, d3.loss_bbox: 1.6152, d4.loss_cls: 0.2605, d4.loss_bbox: 1.7512, loss_cls: 0.2587, loss_bbox: 1.6825, loss: 11.2744, grad_norm: 40.8381
2025-11-19 11:41:19,948 - mmdet - INFO - Iter [750/7752]	lr: 1.966e-04, eta: 2:58:23, time: 1.257, data_time: 0.051, memory: 29766, d0.loss_cls: 0.2508, d0.loss_bbox: 1.5094, d1.loss_cls: 0.2610, d1.loss_bbox: 1.5119, d2.loss_cls: 0.2693, d2.loss_bbox: 1.5262, d3.loss_cls: 0.2695, d3.loss_bbox: 1.5151, d4.loss_cls: 0.2687, d4.loss_bbox: 1.5848, loss_cls: 0.2677, loss_bbox: 1.5414, loss: 10.7756, grad_norm: 39.4480
2025-11-19 11:42:22,692 - mmdet - INFO - Iter [800/7752]	lr: 1.966e-04, eta: 2:55:08, time: 1.255, data_time: 0.051, memory: 29766, d0.loss_cls: 0.2194, d0.loss_bbox: 1.4564, d1.loss_cls: 0.2311, d1.loss_bbox: 1.4827, d2.loss_cls: 0.2375, d2.loss_bbox: 1.5168, d3.loss_cls: 0.2343, d3.loss_bbox: 1.5765, d4.loss_cls: 0.2334, d4.loss_bbox: 1.5922, loss_cls: 0.2426, loss_bbox: 1.5532, loss: 10.5760, grad_norm: 42.1540
2025-11-19 11:43:25,657 - mmdet - INFO - Iter [850/7752]	lr: 1.966e-04, eta: 2:52:10, time: 1.259, data_time: 0.053, memory: 29766, d0.loss_cls: 0.2584, d0.loss_bbox: 1.4584, d1.loss_cls: 0.2709, d1.loss_bbox: 1.4871, d2.loss_cls: 0.2749, d2.loss_bbox: 1.5503, d3.loss_cls: 0.2735, d3.loss_bbox: 1.5390, d4.loss_cls: 0.2740, d4.loss_bbox: 1.5161, loss_cls: 0.2753, loss_bbox: 1.5064, loss: 10.6843, grad_norm: 36.7654
2025-11-19 11:44:28,449 - mmdet - INFO - Iter [900/7752]	lr: 1.966e-04, eta: 2:49:23, time: 1.256, data_time: 0.053, memory: 29766, d0.loss_cls: 0.2189, d0.loss_bbox: 1.4368, d1.loss_cls: 0.2231, d1.loss_bbox: 1.4395, d2.loss_cls: 0.2271, d2.loss_bbox: 1.4530, d3.loss_cls: 0.2321, d3.loss_bbox: 1.5531, d4.loss_cls: 0.2331, d4.loss_bbox: 1.5831, loss_cls: 0.2247, loss_bbox: 1.5936, loss: 10.4181, grad_norm: 42.2523
2025-11-19 11:45:31,040 - mmdet - INFO - Iter [950/7752]	lr: 1.966e-04, eta: 2:46:46, time: 1.252, data_time: 0.052, memory: 29766, d0.loss_cls: 0.2061, d0.loss_bbox: 1.4622, d1.loss_cls: 0.2157, d1.loss_bbox: 1.4688, d2.loss_cls: 0.2170, d2.loss_bbox: 1.4934, d3.loss_cls: 0.2171, d3.loss_bbox: 1.4931, d4.loss_cls: 0.2231, d4.loss_bbox: 1.5987, loss_cls: 0.2172, loss_bbox: 1.5664, loss: 10.3788, grad_norm: 41.5084
2025-11-19 11:45:54,904 - mmdet - INFO - Iter [969/7752]	lr: 1.966e-04, eta: 2:50:22, time: 1.255, data_time: 0.052, memory: 29766, d0.loss_cls: 0.2121, d0.loss_bbox: 1.4331, d1.loss_cls: 0.2233, d1.loss_bbox: 1.4528, d2.loss_cls: 0.2263, d2.loss_bbox: 1.4752, d3.loss_cls: 0.2253, d3.loss_bbox: 1.4707, d4.loss_cls: 0.2265, d4.loss_bbox: 1.5135, loss_cls: 0.2291, loss_bbox: 1.4941, loss: 10.1821, grad_norm: 38.8816
2025-11-19 11:45:54,904 - mmdet - INFO - Saving checkpoint at 3 epochs
2025-11-19 11:46:40,112 - mmdet - INFO - Iter [1000/7752]	lr: 1.924e-04, eta: 2:52:19, time: 1.421, data_time: 0.216, memory: 29766, d0.loss_cls: 0.1768, d0.loss_bbox: 1.4007, d1.loss_cls: 0.1876, d1.loss_bbox: 1.4162, d2.loss_cls: 0.1903, d2.loss_bbox: 1.4234, d3.loss_cls: 0.1939, d3.loss_bbox: 1.4934, d4.loss_cls: 0.1896, d4.loss_bbox: 1.4824, loss_cls: 0.1899, loss_bbox: 1.4661, loss: 9.8104, grad_norm: 42.6645
2025-11-19 11:47:42,876 - mmdet - INFO - Iter [1050/7752]	lr: 1.924e-04, eta: 2:49:35, time: 1.255, data_time: 0.052, memory: 29766, d0.loss_cls: 0.2014, d0.loss_bbox: 1.3336, d1.loss_cls: 0.2108, d1.loss_bbox: 1.3540, d2.loss_cls: 0.2154, d2.loss_bbox: 1.3921, d3.loss_cls: 0.2173, d3.loss_bbox: 1.4240, d4.loss_cls: 0.2173, d4.loss_bbox: 1.4149, loss_cls: 0.2138, loss_bbox: 1.4172, loss: 9.6118, grad_norm: 39.6744
2025-11-19 11:48:45,600 - mmdet - INFO - Iter [1100/7752]	lr: 1.924e-04, eta: 2:46:59, time: 1.254, data_time: 0.052, memory: 29766, d0.loss_cls: 0.2225, d0.loss_bbox: 1.3545, d1.loss_cls: 0.2313, d1.loss_bbox: 1.3577, d2.loss_cls: 0.2348, d2.loss_bbox: 1.3863, d3.loss_cls: 0.2367, d3.loss_bbox: 1.4027, d4.loss_cls: 0.2360, d4.loss_bbox: 1.4288, loss_cls: 0.2364, loss_bbox: 1.4206, loss: 9.7483, grad_norm: 37.9993
2025-11-19 11:49:48,437 - mmdet - INFO - Iter [1150/7752]	lr: 1.924e-04, eta: 2:44:32, time: 1.257, data_time: 0.054, memory: 29766, d0.loss_cls: 0.2043, d0.loss_bbox: 1.3020, d1.loss_cls: 0.2151, d1.loss_bbox: 1.3010, d2.loss_cls: 0.2168, d2.loss_bbox: 1.3505, d3.loss_cls: 0.2186, d3.loss_bbox: 1.3721, d4.loss_cls: 0.2211, d4.loss_bbox: 1.3966, loss_cls: 0.2206, loss_bbox: 1.4163, loss: 9.4351, grad_norm: 39.9770
2025-11-19 11:50:51,339 - mmdet - INFO - Iter [1200/7752]	lr: 1.924e-04, eta: 2:42:13, time: 1.258, data_time: 0.052, memory: 29766, d0.loss_cls: 0.2044, d0.loss_bbox: 1.3295, d1.loss_cls: 0.2149, d1.loss_bbox: 1.3604, d2.loss_cls: 0.2164, d2.loss_bbox: 1.4163, d3.loss_cls: 0.2176, d3.loss_bbox: 1.4328, d4.loss_cls: 0.2181, d4.loss_bbox: 1.4557, loss_cls: 0.2148, loss_bbox: 1.4480, loss: 9.7290, grad_norm: 40.0407
2025-11-19 11:51:53,878 - mmdet - INFO - Iter [1250/7752]	lr: 1.924e-04, eta: 2:39:57, time: 1.251, data_time: 0.051, memory: 29766, d0.loss_cls: 0.2029, d0.loss_bbox: 1.3045, d1.loss_cls: 0.2138, d1.loss_bbox: 1.3024, d2.loss_cls: 0.2190, d2.loss_bbox: 1.3148, d3.loss_cls: 0.2185, d3.loss_bbox: 1.3279, d4.loss_cls: 0.2228, d4.loss_bbox: 1.3554, loss_cls: 0.2177, loss_bbox: 1.3662, loss: 9.2660, grad_norm: 36.7585
2025-11-19 11:52:46,446 - mmdet - INFO - Iter [1292/7752]	lr: 1.924e-04, eta: 2:38:58, time: 1.252, data_time: 0.049, memory: 29766, d0.loss_cls: 0.2099, d0.loss_bbox: 1.3213, d1.loss_cls: 0.2242, d1.loss_bbox: 1.3065, d2.loss_cls: 0.2301, d2.loss_bbox: 1.3110, d3.loss_cls: 0.2286, d3.loss_bbox: 1.3197, d4.loss_cls: 0.2325, d4.loss_bbox: 1.3373, loss_cls: 0.2307, loss_bbox: 1.3492, loss: 9.3011, grad_norm: 33.6217
2025-11-19 11:52:46,446 - mmdet - INFO - Saving checkpoint at 4 epochs
2025-11-19 11:53:02,724 - mmdet - INFO - Iter [1300/7752]	lr: 1.866e-04, eta: 2:45:36, time: 1.885, data_time: 0.679, memory: 29766, d0.loss_cls: 0.2008, d0.loss_bbox: 1.1937, d1.loss_cls: 0.2198, d1.loss_bbox: 1.2427, d2.loss_cls: 0.2258, d2.loss_bbox: 1.3002, d3.loss_cls: 0.2240, d3.loss_bbox: 1.2932, d4.loss_cls: 0.2291, d4.loss_bbox: 1.3196, loss_cls: 0.2253, loss_bbox: 1.3354, loss: 9.0095, grad_norm: 37.0949
2025-11-19 11:54:05,487 - mmdet - INFO - Iter [1350/7752]	lr: 1.866e-04, eta: 2:43:11, time: 1.255, data_time: 0.053, memory: 29766, d0.loss_cls: 0.1773, d0.loss_bbox: 1.3190, d1.loss_cls: 0.1879, d1.loss_bbox: 1.3115, d2.loss_cls: 0.1908, d2.loss_bbox: 1.3302, d3.loss_cls: 0.1916, d3.loss_bbox: 1.3621, d4.loss_cls: 0.1902, d4.loss_bbox: 1.3733, loss_cls: 0.1910, loss_bbox: 1.3661, loss: 9.1910, grad_norm: 37.8856
2025-11-19 11:55:08,167 - mmdet - INFO - Iter [1400/7752]	lr: 1.866e-04, eta: 2:40:52, time: 1.254, data_time: 0.053, memory: 29766, d0.loss_cls: 0.1946, d0.loss_bbox: 1.2545, d1.loss_cls: 0.2077, d1.loss_bbox: 1.2599, d2.loss_cls: 0.2101, d2.loss_bbox: 1.3035, d3.loss_cls: 0.2109, d3.loss_bbox: 1.3033, d4.loss_cls: 0.2114, d4.loss_bbox: 1.3332, loss_cls: 0.2125, loss_bbox: 1.3359, loss: 9.0374, grad_norm: 36.1819
2025-11-19 11:56:10,874 - mmdet - INFO - Iter [1450/7752]	lr: 1.866e-04, eta: 2:38:38, time: 1.254, data_time: 0.052, memory: 29766, d0.loss_cls: 0.1749, d0.loss_bbox: 1.2317, d1.loss_cls: 0.1885, d1.loss_bbox: 1.2209, d2.loss_cls: 0.1901, d2.loss_bbox: 1.2334, d3.loss_cls: 0.1923, d3.loss_bbox: 1.2586, d4.loss_cls: 0.1954, d4.loss_bbox: 1.2785, loss_cls: 0.1965, loss_bbox: 1.2844, loss: 8.6451, grad_norm: 37.6073
2025-11-19 11:57:13,518 - mmdet - INFO - Iter [1500/7752]	lr: 1.866e-04, eta: 2:36:29, time: 1.253, data_time: 0.052, memory: 29766, d0.loss_cls: 0.1646, d0.loss_bbox: 1.2210, d1.loss_cls: 0.1817, d1.loss_bbox: 1.2178, d2.loss_cls: 0.1851, d2.loss_bbox: 1.2206, d3.loss_cls: 0.1855, d3.loss_bbox: 1.2557, d4.loss_cls: 0.1866, d4.loss_bbox: 1.2783, loss_cls: 0.1872, loss_bbox: 1.2820, loss: 8.5662, grad_norm: 38.2447
2025-11-19 11:58:16,310 - mmdet - INFO - Iter [1550/7752]	lr: 1.866e-04, eta: 2:34:25, time: 1.256, data_time: 0.053, memory: 29766, d0.loss_cls: 0.2120, d0.loss_bbox: 1.2764, d1.loss_cls: 0.2259, d1.loss_bbox: 1.2509, d2.loss_cls: 0.2274, d2.loss_bbox: 1.2458, d3.loss_cls: 0.2274, d3.loss_bbox: 1.2804, d4.loss_cls: 0.2306, d4.loss_bbox: 1.2853, loss_cls: 0.2294, loss_bbox: 1.3044, loss: 8.9960, grad_norm: 34.8958
2025-11-19 11:59:19,129 - mmdet - INFO - Iter [1600/7752]	lr: 1.866e-04, eta: 2:32:25, time: 1.256, data_time: 0.054, memory: 29766, d0.loss_cls: 0.1764, d0.loss_bbox: 1.2163, d1.loss_cls: 0.1926, d1.loss_bbox: 1.2051, d2.loss_cls: 0.1938, d2.loss_bbox: 1.2117, d3.loss_cls: 0.1953, d3.loss_bbox: 1.2365, d4.loss_cls: 0.1970, d4.loss_bbox: 1.2423, loss_cls: 0.1965, loss_bbox: 1.2517, loss: 8.5152, grad_norm: 36.3867
2025-11-19 11:59:37,937 - mmdet - INFO - Iter [1615/7752]	lr: 1.866e-04, eta: 2:34:36, time: 1.257, data_time: 0.053, memory: 29766, d0.loss_cls: 0.1703, d0.loss_bbox: 1.2499, d1.loss_cls: 0.1855, d1.loss_bbox: 1.2427, d2.loss_cls: 0.1862, d2.loss_bbox: 1.2461, d3.loss_cls: 0.1885, d3.loss_bbox: 1.2650, d4.loss_cls: 0.1900, d4.loss_bbox: 1.2720, loss_cls: 0.1902, loss_bbox: 1.2775, loss: 8.6637, grad_norm: 37.5095
2025-11-19 11:59:37,937 - mmdet - INFO - Saving checkpoint at 5 epochs
2025-11-19 12:00:27,638 - mmdet - INFO - Iter [1650/7752]	lr: 1.794e-04, eta: 2:34:44, time: 1.387, data_time: 0.187, memory: 29766, d0.loss_cls: 0.1541, d0.loss_bbox: 1.2539, d1.loss_cls: 0.1668, d1.loss_bbox: 1.2206, d2.loss_cls: 0.1717, d2.loss_bbox: 1.2253, d3.loss_cls: 0.1717, d3.loss_bbox: 1.2505, d4.loss_cls: 0.1723, d4.loss_bbox: 1.2572, loss_cls: 0.1720, loss_bbox: 1.2524, loss: 8.4684, grad_norm: 38.2371
2025-11-19 12:01:30,257 - mmdet - INFO - Iter [1700/7752]	lr: 1.794e-04, eta: 2:32:40, time: 1.252, data_time: 0.050, memory: 29766, d0.loss_cls: 0.1656, d0.loss_bbox: 1.2238, d1.loss_cls: 0.1826, d1.loss_bbox: 1.2273, d2.loss_cls: 0.1840, d2.loss_bbox: 1.2326, d3.loss_cls: 0.1866, d3.loss_bbox: 1.2536, d4.loss_cls: 0.1866, d4.loss_bbox: 1.2449, loss_cls: 0.1886, loss_bbox: 1.2415, loss: 8.5178, grad_norm: 36.0895
2025-11-19 12:02:33,025 - mmdet - INFO - Iter [1750/7752]	lr: 1.794e-04, eta: 2:30:40, time: 1.255, data_time: 0.051, memory: 29766, d0.loss_cls: 0.1687, d0.loss_bbox: 1.2074, d1.loss_cls: 0.1864, d1.loss_bbox: 1.1971, d2.loss_cls: 0.1878, d2.loss_bbox: 1.1926, d3.loss_cls: 0.1918, d3.loss_bbox: 1.2005, d4.loss_cls: 0.1927, d4.loss_bbox: 1.2427, loss_cls: 0.1913, loss_bbox: 1.2522, loss: 8.4112, grad_norm: 37.2523
2025-11-19 12:03:35,657 - mmdet - INFO - Iter [1800/7752]	lr: 1.794e-04, eta: 2:28:43, time: 1.253, data_time: 0.051, memory: 29766, d0.loss_cls: 0.1728, d0.loss_bbox: 1.2055, d1.loss_cls: 0.1911, d1.loss_bbox: 1.1833, d2.loss_cls: 0.1947, d2.loss_bbox: 1.1825, d3.loss_cls: 0.1964, d3.loss_bbox: 1.2184, d4.loss_cls: 0.1968, d4.loss_bbox: 1.2281, loss_cls: 0.1975, loss_bbox: 1.2214, loss: 8.3885, grad_norm: 34.5467
2025-11-19 12:04:38,534 - mmdet - INFO - Iter [1850/7752]	lr: 1.794e-04, eta: 2:26:49, time: 1.258, data_time: 0.053, memory: 29766, d0.loss_cls: 0.1643, d0.loss_bbox: 1.1401, d1.loss_cls: 0.1835, d1.loss_bbox: 1.1486, d2.loss_cls: 0.1862, d2.loss_bbox: 1.1456, d3.loss_cls: 0.1892, d3.loss_bbox: 1.1656, d4.loss_cls: 0.1889, d4.loss_bbox: 1.1717, loss_cls: 0.1894, loss_bbox: 1.1638, loss: 8.0370, grad_norm: 35.1683
2025-11-19 12:05:41,339 - mmdet - INFO - Iter [1900/7752]	lr: 1.794e-04, eta: 2:24:58, time: 1.256, data_time: 0.053, memory: 29766, d0.loss_cls: 0.1535, d0.loss_bbox: 1.1022, d1.loss_cls: 0.1718, d1.loss_bbox: 1.1050, d2.loss_cls: 0.1741, d2.loss_bbox: 1.1164, d3.loss_cls: 0.1755, d3.loss_bbox: 1.1269, d4.loss_cls: 0.1765, d4.loss_bbox: 1.1672, loss_cls: 0.1772, loss_bbox: 1.1568, loss: 7.8030, grad_norm: 35.7626
2025-11-19 12:06:29,113 - mmdet - INFO - Iter [1938/7752]	lr: 1.794e-04, eta: 2:24:21, time: 1.258, data_time: 0.052, memory: 29766, d0.loss_cls: 0.1603, d0.loss_bbox: 1.0899, d1.loss_cls: 0.1774, d1.loss_bbox: 1.0824, d2.loss_cls: 0.1815, d2.loss_bbox: 1.0944, d3.loss_cls: 0.1830, d3.loss_bbox: 1.1046, d4.loss_cls: 0.1858, d4.loss_bbox: 1.1265, loss_cls: 0.1878, loss_bbox: 1.1265, loss: 7.7002, grad_norm: 34.5232
2025-11-19 12:06:29,114 - mmdet - INFO - Saving checkpoint at 6 epochs
2025-11-19 12:06:49,881 - mmdet - INFO - Iter [1950/7752]	lr: 1.707e-04, eta: 2:27:12, time: 1.629, data_time: 0.427, memory: 29766, d0.loss_cls: 0.1440, d0.loss_bbox: 1.1344, d1.loss_cls: 0.1612, d1.loss_bbox: 1.1360, d2.loss_cls: 0.1645, d2.loss_bbox: 1.1587, d3.loss_cls: 0.1647, d3.loss_bbox: 1.1364, d4.loss_cls: 0.1681, d4.loss_bbox: 1.1371, loss_cls: 0.1695, loss_bbox: 1.1209, loss: 7.7957, grad_norm: 33.9238
2025-11-19 12:07:52,475 - mmdet - INFO - Iter [2000/7752]	lr: 1.707e-04, eta: 2:25:17, time: 1.252, data_time: 0.049, memory: 29766, d0.loss_cls: 0.1656, d0.loss_bbox: 1.1495, d1.loss_cls: 0.1824, d1.loss_bbox: 1.1233, d2.loss_cls: 0.1873, d2.loss_bbox: 1.1357, d3.loss_cls: 0.1878, d3.loss_bbox: 1.1366, d4.loss_cls: 0.1904, d4.loss_bbox: 1.1565, loss_cls: 0.1913, loss_bbox: 1.1557, loss: 7.9621, grad_norm: 36.2739
2025-11-19 12:08:55,255 - mmdet - INFO - Iter [2050/7752]	lr: 1.707e-04, eta: 2:23:25, time: 1.256, data_time: 0.051, memory: 29766, d0.loss_cls: 0.1769, d0.loss_bbox: 1.0977, d1.loss_cls: 0.1962, d1.loss_bbox: 1.0765, d2.loss_cls: 0.1996, d2.loss_bbox: 1.0766, d3.loss_cls: 0.2015, d3.loss_bbox: 1.0909, d4.loss_cls: 0.2031, d4.loss_bbox: 1.1070, loss_cls: 0.2054, loss_bbox: 1.1161, loss: 7.7475, grad_norm: 33.8120
2025-11-19 12:09:57,882 - mmdet - INFO - Iter [2100/7752]	lr: 1.707e-04, eta: 2:21:35, time: 1.253, data_time: 0.050, memory: 29766, d0.loss_cls: 0.1622, d0.loss_bbox: 1.1067, d1.loss_cls: 0.1820, d1.loss_bbox: 1.0961, d2.loss_cls: 0.1864, d2.loss_bbox: 1.0989, d3.loss_cls: 0.1889, d3.loss_bbox: 1.1170, d4.loss_cls: 0.1913, d4.loss_bbox: 1.1266, loss_cls: 0.1930, loss_bbox: 1.1242, loss: 7.7733, grad_norm: 34.8752
2025-11-19 12:11:00,549 - mmdet - INFO - Iter [2150/7752]	lr: 1.707e-04, eta: 2:19:48, time: 1.253, data_time: 0.050, memory: 29766, d0.loss_cls: 0.1697, d0.loss_bbox: 1.1366, d1.loss_cls: 0.1842, d1.loss_bbox: 1.1223, d2.loss_cls: 0.1882, d2.loss_bbox: 1.1239, d3.loss_cls: 0.1898, d3.loss_bbox: 1.1282, d4.loss_cls: 0.1916, d4.loss_bbox: 1.1477, loss_cls: 0.1938, loss_bbox: 1.1504, loss: 7.9263, grad_norm: 35.5767
2025-11-19 12:12:03,265 - mmdet - INFO - Iter [2200/7752]	lr: 1.707e-04, eta: 2:18:02, time: 1.254, data_time: 0.053, memory: 29766, d0.loss_cls: 0.1335, d0.loss_bbox: 1.1236, d1.loss_cls: 0.1498, d1.loss_bbox: 1.1187, d2.loss_cls: 0.1540, d2.loss_bbox: 1.1157, d3.loss_cls: 0.1560, d3.loss_bbox: 1.1026, d4.loss_cls: 0.1595, d4.loss_bbox: 1.1195, loss_cls: 0.1585, loss_bbox: 1.1519, loss: 7.6433, grad_norm: 39.1711
2025-11-19 12:13:05,861 - mmdet - INFO - Iter [2250/7752]	lr: 1.707e-04, eta: 2:16:18, time: 1.252, data_time: 0.051, memory: 29768, d0.loss_cls: 0.1335, d0.loss_bbox: 1.0553, d1.loss_cls: 0.1506, d1.loss_bbox: 1.0295, d2.loss_cls: 0.1555, d2.loss_bbox: 1.0244, d3.loss_cls: 0.1576, d3.loss_bbox: 1.0412, d4.loss_cls: 0.1619, d4.loss_bbox: 1.0234, loss_cls: 0.1631, loss_bbox: 1.0305, loss: 7.1263, grad_norm: 37.5522
2025-11-19 12:13:19,645 - mmdet - INFO - Iter [2261/7752]	lr: 1.707e-04, eta: 2:17:54, time: 1.254, data_time: 0.051, memory: 29768, d0.loss_cls: 0.1180, d0.loss_bbox: 1.0283, d1.loss_cls: 0.1410, d1.loss_bbox: 1.0183, d2.loss_cls: 0.1447, d2.loss_bbox: 1.0163, d3.loss_cls: 0.1481, d3.loss_bbox: 1.0379, d4.loss_cls: 0.1520, d4.loss_bbox: 1.0181, loss_cls: 0.1523, loss_bbox: 1.0158, loss: 6.9909, grad_norm: 39.4174
2025-11-19 12:13:19,646 - mmdet - INFO - Saving checkpoint at 7 epochs
2025-11-19 12:14:15,681 - mmdet - INFO - Iter [2300/7752]	lr: 1.609e-04, eta: 2:17:23, time: 1.407, data_time: 0.200, memory: 29768, d0.loss_cls: 0.1419, d0.loss_bbox: 1.0902, d1.loss_cls: 0.1611, d1.loss_bbox: 1.0667, d2.loss_cls: 0.1702, d2.loss_bbox: 1.0949, d3.loss_cls: 0.1707, d3.loss_bbox: 1.1207, d4.loss_cls: 0.1744, d4.loss_bbox: 1.0939, loss_cls: 0.1755, loss_bbox: 1.1005, loss: 7.5607, grad_norm: 36.1931
2025-11-19 12:15:18,668 - mmdet - INFO - Iter [2350/7752]	lr: 1.609e-04, eta: 2:15:38, time: 1.260, data_time: 0.054, memory: 29768, d0.loss_cls: 0.1385, d0.loss_bbox: 1.0580, d1.loss_cls: 0.1570, d1.loss_bbox: 1.0531, d2.loss_cls: 0.1617, d2.loss_bbox: 1.0478, d3.loss_cls: 0.1652, d3.loss_bbox: 1.0549, d4.loss_cls: 0.1672, d4.loss_bbox: 1.0593, loss_cls: 0.1684, loss_bbox: 1.0536, loss: 7.2847, grad_norm: 36.2657
2025-11-19 12:16:21,498 - mmdet - INFO - Iter [2400/7752]	lr: 1.609e-04, eta: 2:13:55, time: 1.257, data_time: 0.053, memory: 29768, d0.loss_cls: 0.1290, d0.loss_bbox: 1.0460, d1.loss_cls: 0.1507, d1.loss_bbox: 1.0225, d2.loss_cls: 0.1560, d2.loss_bbox: 1.0313, d3.loss_cls: 0.1594, d3.loss_bbox: 1.0105, d4.loss_cls: 0.1618, d4.loss_bbox: 1.0229, loss_cls: 0.1640, loss_bbox: 1.0382, loss: 7.0923, grad_norm: 38.1938
2025-11-19 12:17:24,635 - mmdet - INFO - Iter [2450/7752]	lr: 1.609e-04, eta: 2:12:14, time: 1.263, data_time: 0.056, memory: 29768, d0.loss_cls: 0.1653, d0.loss_bbox: 1.0604, d1.loss_cls: 0.1838, d1.loss_bbox: 1.0432, d2.loss_cls: 0.1886, d2.loss_bbox: 1.0404, d3.loss_cls: 0.1922, d3.loss_bbox: 1.0307, d4.loss_cls: 0.1960, d4.loss_bbox: 1.0404, loss_cls: 0.1981, loss_bbox: 1.0494, loss: 7.3885, grad_norm: 35.6769
2025-11-19 12:18:27,659 - mmdet - INFO - Iter [2500/7752]	lr: 1.609e-04, eta: 2:10:35, time: 1.260, data_time: 0.056, memory: 29768, d0.loss_cls: 0.1293, d0.loss_bbox: 1.0144, d1.loss_cls: 0.1523, d1.loss_bbox: 0.9966, d2.loss_cls: 0.1586, d2.loss_bbox: 0.9990, d3.loss_cls: 0.1598, d3.loss_bbox: 1.0025, d4.loss_cls: 0.1632, d4.loss_bbox: 1.0219, loss_cls: 0.1661, loss_bbox: 1.0219, loss: 6.9857, grad_norm: 36.4281
2025-11-19 12:19:30,773 - mmdet - INFO - Iter [2550/7752]	lr: 1.609e-04, eta: 2:08:57, time: 1.262, data_time: 0.056, memory: 29768, d0.loss_cls: 0.1381, d0.loss_bbox: 0.9936, d1.loss_cls: 0.1581, d1.loss_bbox: 0.9726, d2.loss_cls: 0.1676, d2.loss_bbox: 0.9605, d3.loss_cls: 0.1691, d3.loss_bbox: 0.9706, d4.loss_cls: 0.1744, d4.loss_bbox: 0.9761, loss_cls: 0.1761, loss_bbox: 0.9901, loss: 6.8469, grad_norm: 35.5131
2025-11-19 12:20:13,445 - mmdet - INFO - Iter [2584/7752]	lr: 1.609e-04, eta: 2:08:31, time: 1.257, data_time: 0.053, memory: 29768, d0.loss_cls: 0.1422, d0.loss_bbox: 1.0562, d1.loss_cls: 0.1560, d1.loss_bbox: 1.0057, d2.loss_cls: 0.1631, d2.loss_bbox: 0.9966, d3.loss_cls: 0.1650, d3.loss_bbox: 1.0004, d4.loss_cls: 0.1685, d4.loss_bbox: 1.0004, loss_cls: 0.1710, loss_bbox: 1.0095, loss: 7.0346, grad_norm: 37.3323
2025-11-19 12:20:13,446 - mmdet - INFO - Saving checkpoint at 8 epochs
2025-11-19 12:20:40,368 - mmdet - INFO - Iter [2600/7752]	lr: 1.501e-04, eta: 2:09:57, time: 1.594, data_time: 0.377, memory: 29768, d0.loss_cls: 0.1300, d0.loss_bbox: 1.0089, d1.loss_cls: 0.1437, d1.loss_bbox: 0.9958, d2.loss_cls: 0.1547, d2.loss_bbox: 0.9754, d3.loss_cls: 0.1571, d3.loss_bbox: 0.9545, d4.loss_cls: 0.1592, d4.loss_bbox: 0.9618, loss_cls: 0.1600, loss_bbox: 0.9723, loss: 6.7733, grad_norm: 37.2509
2025-11-19 12:21:43,463 - mmdet - INFO - Iter [2650/7752]	lr: 1.501e-04, eta: 2:08:18, time: 1.262, data_time: 0.055, memory: 29768, d0.loss_cls: 0.1272, d0.loss_bbox: 0.9550, d1.loss_cls: 0.1499, d1.loss_bbox: 0.9491, d2.loss_cls: 0.1616, d2.loss_bbox: 0.9386, d3.loss_cls: 0.1647, d3.loss_bbox: 0.9344, d4.loss_cls: 0.1670, d4.loss_bbox: 0.9537, loss_cls: 0.1695, loss_bbox: 0.9423, loss: 6.6130, grad_norm: 34.0478
2025-11-19 12:22:46,172 - mmdet - INFO - Iter [2700/7752]	lr: 1.501e-04, eta: 2:06:38, time: 1.254, data_time: 0.052, memory: 29768, d0.loss_cls: 0.1321, d0.loss_bbox: 1.0221, d1.loss_cls: 0.1500, d1.loss_bbox: 1.0012, d2.loss_cls: 0.1608, d2.loss_bbox: 0.9716, d3.loss_cls: 0.1626, d3.loss_bbox: 0.9969, d4.loss_cls: 0.1670, d4.loss_bbox: 1.0099, loss_cls: 0.1690, loss_bbox: 0.9966, loss: 6.9398, grad_norm: 37.4221
2025-11-19 12:23:49,121 - mmdet - INFO - Iter [2750/7752]	lr: 1.501e-04, eta: 2:05:01, time: 1.259, data_time: 0.051, memory: 29768, d0.loss_cls: 0.1281, d0.loss_bbox: 1.0396, d1.loss_cls: 0.1443, d1.loss_bbox: 0.9864, d2.loss_cls: 0.1554, d2.loss_bbox: 0.9720, d3.loss_cls: 0.1586, d3.loss_bbox: 0.9672, d4.loss_cls: 0.1613, d4.loss_bbox: 0.9797, loss_cls: 0.1646, loss_bbox: 0.9782, loss: 6.8353, grad_norm: 36.7343
2025-11-19 12:24:52,089 - mmdet - INFO - Iter [2800/7752]	lr: 1.501e-04, eta: 2:03:25, time: 1.259, data_time: 0.053, memory: 29768, d0.loss_cls: 0.1273, d0.loss_bbox: 1.0032, d1.loss_cls: 0.1426, d1.loss_bbox: 0.9816, d2.loss_cls: 0.1534, d2.loss_bbox: 0.9749, d3.loss_cls: 0.1566, d3.loss_bbox: 0.9671, d4.loss_cls: 0.1601, d4.loss_bbox: 0.9818, loss_cls: 0.1639, loss_bbox: 0.9659, loss: 6.7784, grad_norm: 38.4055
2025-11-19 12:25:54,901 - mmdet - INFO - Iter [2850/7752]	lr: 1.501e-04, eta: 2:01:49, time: 1.256, data_time: 0.053, memory: 29768, d0.loss_cls: 0.1310, d0.loss_bbox: 1.0117, d1.loss_cls: 0.1486, d1.loss_bbox: 0.9680, d2.loss_cls: 0.1569, d2.loss_bbox: 0.9556, d3.loss_cls: 0.1601, d3.loss_bbox: 0.9529, d4.loss_cls: 0.1654, d4.loss_bbox: 0.9780, loss_cls: 0.1681, loss_bbox: 1.0045, loss: 6.8008, grad_norm: 36.7424
2025-11-19 12:26:57,770 - mmdet - INFO - Iter [2900/7752]	lr: 1.501e-04, eta: 2:00:15, time: 1.257, data_time: 0.051, memory: 29768, d0.loss_cls: 0.1304, d0.loss_bbox: 0.9938, d1.loss_cls: 0.1469, d1.loss_bbox: 0.9595, d2.loss_cls: 0.1540, d2.loss_bbox: 0.9491, d3.loss_cls: 0.1567, d3.loss_bbox: 0.9546, d4.loss_cls: 0.1595, d4.loss_bbox: 0.9611, loss_cls: 0.1625, loss_bbox: 0.9697, loss: 6.6980, grad_norm: 37.5488
2025-11-19 12:27:06,475 - mmdet - INFO - Iter [2907/7752]	lr: 1.501e-04, eta: 2:01:32, time: 1.256, data_time: 0.049, memory: 29768, d0.loss_cls: 0.1336, d0.loss_bbox: 0.9968, d1.loss_cls: 0.1492, d1.loss_bbox: 0.9691, d2.loss_cls: 0.1579, d2.loss_bbox: 0.9594, d3.loss_cls: 0.1602, d3.loss_bbox: 0.9600, d4.loss_cls: 0.1633, d4.loss_bbox: 0.9662, loss_cls: 0.1662, loss_bbox: 0.9795, loss: 6.7616, grad_norm: 37.5169
2025-11-19 12:27:06,476 - mmdet - INFO - Saving checkpoint at 9 epochs
2025-11-19 12:28:06,817 - mmdet - INFO - Iter [2950/7752]	lr: 1.383e-04, eta: 2:00:34, time: 1.372, data_time: 0.167, memory: 29768, d0.loss_cls: 0.1158, d0.loss_bbox: 0.9973, d1.loss_cls: 0.1282, d1.loss_bbox: 0.9490, d2.loss_cls: 0.1390, d2.loss_bbox: 0.9292, d3.loss_cls: 0.1432, d3.loss_bbox: 0.9249, d4.loss_cls: 0.1462, d4.loss_bbox: 0.9311, loss_cls: 0.1484, loss_bbox: 0.9394, loss: 6.4916, grad_norm: 35.7232
2025-11-19 12:29:09,614 - mmdet - INFO - Iter [3000/7752]	lr: 1.383e-04, eta: 1:58:58, time: 1.256, data_time: 0.049, memory: 29768, d0.loss_cls: 0.1253, d0.loss_bbox: 0.9975, d1.loss_cls: 0.1424, d1.loss_bbox: 0.9686, d2.loss_cls: 0.1515, d2.loss_bbox: 0.9376, d3.loss_cls: 0.1565, d3.loss_bbox: 0.9441, d4.loss_cls: 0.1587, d4.loss_bbox: 0.9486, loss_cls: 0.1609, loss_bbox: 0.9603, loss: 6.6520, grad_norm: 37.6188
2025-11-19 12:30:12,302 - mmdet - INFO - Iter [3050/7752]	lr: 1.383e-04, eta: 1:57:24, time: 1.254, data_time: 0.049, memory: 29768, d0.loss_cls: 0.1357, d0.loss_bbox: 1.0114, d1.loss_cls: 0.1511, d1.loss_bbox: 0.9781, d2.loss_cls: 0.1605, d2.loss_bbox: 0.9677, d3.loss_cls: 0.1629, d3.loss_bbox: 0.9601, d4.loss_cls: 0.1672, d4.loss_bbox: 0.9658, loss_cls: 0.1681, loss_bbox: 0.9778, loss: 6.8063, grad_norm: 36.4331
2025-11-19 12:31:15,137 - mmdet - INFO - Iter [3100/7752]	lr: 1.383e-04, eta: 1:55:51, time: 1.257, data_time: 0.050, memory: 29768, d0.loss_cls: 0.1133, d0.loss_bbox: 0.9790, d1.loss_cls: 0.1292, d1.loss_bbox: 0.9393, d2.loss_cls: 0.1405, d2.loss_bbox: 0.9121, d3.loss_cls: 0.1446, d3.loss_bbox: 0.9141, d4.loss_cls: 0.1482, d4.loss_bbox: 0.9191, loss_cls: 0.1501, loss_bbox: 0.9175, loss: 6.4069, grad_norm: 38.1026
2025-11-19 12:32:18,039 - mmdet - INFO - Iter [3150/7752]	lr: 1.383e-04, eta: 1:54:19, time: 1.258, data_time: 0.050, memory: 29768, d0.loss_cls: 0.1133, d0.loss_bbox: 0.9199, d1.loss_cls: 0.1313, d1.loss_bbox: 0.8836, d2.loss_cls: 0.1463, d2.loss_bbox: 0.8615, d3.loss_cls: 0.1494, d3.loss_bbox: 0.8637, d4.loss_cls: 0.1549, d4.loss_bbox: 0.8698, loss_cls: 0.1580, loss_bbox: 0.8741, loss: 6.1260, grad_norm: 34.3808
2025-11-19 12:33:20,694 - mmdet - INFO - Iter [3200/7752]	lr: 1.383e-04, eta: 1:52:48, time: 1.253, data_time: 0.047, memory: 29768, d0.loss_cls: 0.1157, d0.loss_bbox: 0.9510, d1.loss_cls: 0.1333, d1.loss_bbox: 0.9108, d2.loss_cls: 0.1431, d2.loss_bbox: 0.8957, d3.loss_cls: 0.1486, d3.loss_bbox: 0.8916, d4.loss_cls: 0.1539, d4.loss_bbox: 0.8894, loss_cls: 0.1555, loss_bbox: 0.8835, loss: 6.2719, grad_norm: 39.7845
2025-11-19 12:33:58,307 - mmdet - INFO - Iter [3230/7752]	lr: 1.383e-04, eta: 1:52:28, time: 1.253, data_time: 0.047, memory: 29768, d0.loss_cls: 0.1231, d0.loss_bbox: 0.9366, d1.loss_cls: 0.1405, d1.loss_bbox: 0.9001, d2.loss_cls: 0.1535, d2.loss_bbox: 0.8774, d3.loss_cls: 0.1589, d3.loss_bbox: 0.8689, d4.loss_cls: 0.1638, d4.loss_bbox: 0.8685, loss_cls: 0.1656, loss_bbox: 0.8768, loss: 6.2337, grad_norm: 36.7606
2025-11-19 12:33:58,307 - mmdet - INFO - Saving checkpoint at 10 epochs
2025-11-19 12:34:30,750 - mmdet - INFO - Iter [3250/7752]	lr: 1.260e-04, eta: 1:53:05, time: 1.556, data_time: 0.348, memory: 29768, d0.loss_cls: 0.1305, d0.loss_bbox: 0.9346, d1.loss_cls: 0.1436, d1.loss_bbox: 0.9251, d2.loss_cls: 0.1595, d2.loss_bbox: 0.8678, d3.loss_cls: 0.1652, d3.loss_bbox: 0.8726, d4.loss_cls: 0.1687, d4.loss_bbox: 0.8846, loss_cls: 0.1723, loss_bbox: 0.8749, loss: 6.2995, grad_norm: 32.0607
2025-11-19 12:35:33,585 - mmdet - INFO - Iter [3300/7752]	lr: 1.260e-04, eta: 1:51:33, time: 1.257, data_time: 0.050, memory: 29768, d0.loss_cls: 0.1219, d0.loss_bbox: 0.9136, d1.loss_cls: 0.1336, d1.loss_bbox: 0.8878, d2.loss_cls: 0.1468, d2.loss_bbox: 0.8708, d3.loss_cls: 0.1528, d3.loss_bbox: 0.8811, d4.loss_cls: 0.1555, d4.loss_bbox: 0.8885, loss_cls: 0.1577, loss_bbox: 0.8921, loss: 6.2021, grad_norm: 36.0749
2025-11-19 12:36:36,355 - mmdet - INFO - Iter [3350/7752]	lr: 1.260e-04, eta: 1:50:01, time: 1.255, data_time: 0.048, memory: 29768, d0.loss_cls: 0.1101, d0.loss_bbox: 0.9011, d1.loss_cls: 0.1207, d1.loss_bbox: 0.8716, d2.loss_cls: 0.1348, d2.loss_bbox: 0.8565, d3.loss_cls: 0.1420, d3.loss_bbox: 0.8553, d4.loss_cls: 0.1468, d4.loss_bbox: 0.8565, loss_cls: 0.1490, loss_bbox: 0.8618, loss: 6.0062, grad_norm: 36.6176
2025-11-19 12:37:39,212 - mmdet - INFO - Iter [3400/7752]	lr: 1.260e-04, eta: 1:48:31, time: 1.257, data_time: 0.050, memory: 29768, d0.loss_cls: 0.1076, d0.loss_bbox: 0.9424, d1.loss_cls: 0.1174, d1.loss_bbox: 0.8949, d2.loss_cls: 0.1293, d2.loss_bbox: 0.8862, d3.loss_cls: 0.1367, d3.loss_bbox: 0.8822, d4.loss_cls: 0.1394, d4.loss_bbox: 0.8848, loss_cls: 0.1419, loss_bbox: 0.8902, loss: 6.1530, grad_norm: 37.3428
2025-11-19 12:38:42,004 - mmdet - INFO - Iter [3450/7752]	lr: 1.260e-04, eta: 1:47:01, time: 1.256, data_time: 0.048, memory: 29768, d0.loss_cls: 0.1236, d0.loss_bbox: 0.9691, d1.loss_cls: 0.1344, d1.loss_bbox: 0.9351, d2.loss_cls: 0.1478, d2.loss_bbox: 0.9228, d3.loss_cls: 0.1567, d3.loss_bbox: 0.9178, d4.loss_cls: 0.1596, d4.loss_bbox: 0.9150, loss_cls: 0.1629, loss_bbox: 0.9264, loss: 6.4712, grad_norm: 35.4658
2025-11-19 12:39:44,668 - mmdet - INFO - Iter [3500/7752]	lr: 1.260e-04, eta: 1:45:32, time: 1.253, data_time: 0.048, memory: 29768, d0.loss_cls: 0.1010, d0.loss_bbox: 0.8557, d1.loss_cls: 0.1126, d1.loss_bbox: 0.8407, d2.loss_cls: 0.1309, d2.loss_bbox: 0.8301, d3.loss_cls: 0.1418, d3.loss_bbox: 0.8423, d4.loss_cls: 0.1465, d4.loss_bbox: 0.8525, loss_cls: 0.1488, loss_bbox: 0.8351, loss: 5.8378, grad_norm: 38.1578
2025-11-19 12:40:47,537 - mmdet - INFO - Iter [3550/7752]	lr: 1.260e-04, eta: 1:44:04, time: 1.257, data_time: 0.048, memory: 29768, d0.loss_cls: 0.1184, d0.loss_bbox: 0.9586, d1.loss_cls: 0.1298, d1.loss_bbox: 0.9249, d2.loss_cls: 0.1430, d2.loss_bbox: 0.8936, d3.loss_cls: 0.1509, d3.loss_bbox: 0.8934, d4.loss_cls: 0.1543, d4.loss_bbox: 0.8996, loss_cls: 0.1572, loss_bbox: 0.8933, loss: 6.3170, grad_norm: 37.3429
2025-11-19 12:40:51,258 - mmdet - INFO - Iter [3553/7752]	lr: 1.260e-04, eta: 1:45:08, time: 1.256, data_time: 0.047, memory: 29768, d0.loss_cls: 0.1168, d0.loss_bbox: 0.9603, d1.loss_cls: 0.1281, d1.loss_bbox: 0.9266, d2.loss_cls: 0.1414, d2.loss_bbox: 0.8930, d3.loss_cls: 0.1492, d3.loss_bbox: 0.8965, d4.loss_cls: 0.1526, d4.loss_bbox: 0.9003, loss_cls: 0.1559, loss_bbox: 0.8940, loss: 6.3146, grad_norm: 37.2136
2025-11-19 12:40:51,259 - mmdet - INFO - Saving checkpoint at 11 epochs
2025-11-19 12:41:57,112 - mmdet - INFO - Iter [3600/7752]	lr: 1.131e-04, eta: 1:43:55, time: 1.374, data_time: 0.170, memory: 29768, d0.loss_cls: 0.0953, d0.loss_bbox: 0.8931, d1.loss_cls: 0.1082, d1.loss_bbox: 0.8551, d2.loss_cls: 0.1222, d2.loss_bbox: 0.8295, d3.loss_cls: 0.1315, d3.loss_bbox: 0.8218, d4.loss_cls: 0.1379, d4.loss_bbox: 0.8274, loss_cls: 0.1393, loss_bbox: 0.8389, loss: 5.8004, grad_norm: 37.9238
2025-11-19 12:42:59,715 - mmdet - INFO - Iter [3650/7752]	lr: 1.131e-04, eta: 1:42:26, time: 1.252, data_time: 0.047, memory: 29768, d0.loss_cls: 0.1243, d0.loss_bbox: 0.9225, d1.loss_cls: 0.1344, d1.loss_bbox: 0.8889, d2.loss_cls: 0.1501, d2.loss_bbox: 0.8742, d3.loss_cls: 0.1586, d3.loss_bbox: 0.8698, d4.loss_cls: 0.1641, d4.loss_bbox: 0.8767, loss_cls: 0.1667, loss_bbox: 0.8746, loss: 6.2048, grad_norm: 32.7583
2025-11-19 12:44:02,516 - mmdet - INFO - Iter [3700/7752]	lr: 1.131e-04, eta: 1:40:58, time: 1.256, data_time: 0.049, memory: 29768, d0.loss_cls: 0.1153, d0.loss_bbox: 0.8785, d1.loss_cls: 0.1253, d1.loss_bbox: 0.8358, d2.loss_cls: 0.1405, d2.loss_bbox: 0.8117, d3.loss_cls: 0.1513, d3.loss_bbox: 0.8062, d4.loss_cls: 0.1557, d4.loss_bbox: 0.8049, loss_cls: 0.1576, loss_bbox: 0.8053, loss: 5.7881, grad_norm: 36.0957
2025-11-19 12:45:05,352 - mmdet - INFO - Iter [3750/7752]	lr: 1.131e-04, eta: 1:39:30, time: 1.257, data_time: 0.049, memory: 29768, d0.loss_cls: 0.1030, d0.loss_bbox: 0.9051, d1.loss_cls: 0.1123, d1.loss_bbox: 0.8591, d2.loss_cls: 0.1270, d2.loss_bbox: 0.8295, d3.loss_cls: 0.1364, d3.loss_bbox: 0.8116, d4.loss_cls: 0.1395, d4.loss_bbox: 0.8238, loss_cls: 0.1429, loss_bbox: 0.8178, loss: 5.8080, grad_norm: 35.5617
2025-11-19 12:46:08,093 - mmdet - INFO - Iter [3800/7752]	lr: 1.131e-04, eta: 1:38:04, time: 1.255, data_time: 0.048, memory: 29768, d0.loss_cls: 0.1039, d0.loss_bbox: 0.8760, d1.loss_cls: 0.1134, d1.loss_bbox: 0.8462, d2.loss_cls: 0.1271, d2.loss_bbox: 0.8187, d3.loss_cls: 0.1374, d3.loss_bbox: 0.8060, d4.loss_cls: 0.1409, d4.loss_bbox: 0.8185, loss_cls: 0.1439, loss_bbox: 0.8223, loss: 5.7541, grad_norm: 35.1515
2025-11-19 12:47:10,783 - mmdet - INFO - Iter [3850/7752]	lr: 1.131e-04, eta: 1:36:37, time: 1.254, data_time: 0.048, memory: 29768, d0.loss_cls: 0.1038, d0.loss_bbox: 0.9129, d1.loss_cls: 0.1129, d1.loss_bbox: 0.8800, d2.loss_cls: 0.1282, d2.loss_bbox: 0.8384, d3.loss_cls: 0.1385, d3.loss_bbox: 0.8386, d4.loss_cls: 0.1443, d4.loss_bbox: 0.8475, loss_cls: 0.1480, loss_bbox: 0.8361, loss: 5.9292, grad_norm: 37.9972
2025-11-19 12:47:43,495 - mmdet - INFO - Iter [3876/7752]	lr: 1.131e-04, eta: 1:36:23, time: 1.257, data_time: 0.047, memory: 29768, d0.loss_cls: 0.0962, d0.loss_bbox: 0.8524, d1.loss_cls: 0.1077, d1.loss_bbox: 0.8275, d2.loss_cls: 0.1225, d2.loss_bbox: 0.7919, d3.loss_cls: 0.1358, d3.loss_bbox: 0.7929, d4.loss_cls: 0.1421, d4.loss_bbox: 0.7958, loss_cls: 0.1448, loss_bbox: 0.7970, loss: 5.6065, grad_norm: 38.2831
2025-11-19 12:47:43,496 - mmdet - INFO - Saving checkpoint at 12 epochs
2025-11-19 12:48:20,500 - mmdet - INFO - Iter [3900/7752]	lr: 1.001e-04, eta: 1:36:25, time: 1.488, data_time: 0.277, memory: 29768, d0.loss_cls: 0.1138, d0.loss_bbox: 0.8627, d1.loss_cls: 0.1251, d1.loss_bbox: 0.8255, d2.loss_cls: 0.1428, d2.loss_bbox: 0.7998, d3.loss_cls: 0.1542, d3.loss_bbox: 0.7826, d4.loss_cls: 0.1581, d4.loss_bbox: 0.7929, loss_cls: 0.1608, loss_bbox: 0.7950, loss: 5.7133, grad_norm: 33.8665
2025-11-19 12:49:23,335 - mmdet - INFO - Iter [3950/7752]	lr: 1.001e-04, eta: 1:34:58, time: 1.257, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0996, d0.loss_bbox: 0.8308, d1.loss_cls: 0.1086, d1.loss_bbox: 0.8110, d2.loss_cls: 0.1247, d2.loss_bbox: 0.7932, d3.loss_cls: 0.1412, d3.loss_bbox: 0.7876, d4.loss_cls: 0.1466, d4.loss_bbox: 0.7930, loss_cls: 0.1482, loss_bbox: 0.7924, loss: 5.5770, grad_norm: 36.0676
2025-11-19 12:50:26,350 - mmdet - INFO - Iter [4000/7752]	lr: 1.001e-04, eta: 1:33:32, time: 1.260, data_time: 0.049, memory: 29768, d0.loss_cls: 0.1074, d0.loss_bbox: 0.8968, d1.loss_cls: 0.1139, d1.loss_bbox: 0.8540, d2.loss_cls: 0.1296, d2.loss_bbox: 0.8297, d3.loss_cls: 0.1413, d3.loss_bbox: 0.8237, d4.loss_cls: 0.1462, d4.loss_bbox: 0.8275, loss_cls: 0.1481, loss_bbox: 0.8280, loss: 5.8461, grad_norm: 35.7733
2025-11-19 12:51:29,142 - mmdet - INFO - Iter [4050/7752]	lr: 1.001e-04, eta: 1:32:06, time: 1.256, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0986, d0.loss_bbox: 0.8884, d1.loss_cls: 0.1062, d1.loss_bbox: 0.8475, d2.loss_cls: 0.1162, d2.loss_bbox: 0.8237, d3.loss_cls: 0.1275, d3.loss_bbox: 0.8182, d4.loss_cls: 0.1346, d4.loss_bbox: 0.8239, loss_cls: 0.1376, loss_bbox: 0.8169, loss: 5.7394, grad_norm: 37.3883
2025-11-19 12:52:32,108 - mmdet - INFO - Iter [4100/7752]	lr: 1.001e-04, eta: 1:30:41, time: 1.259, data_time: 0.049, memory: 29768, d0.loss_cls: 0.1003, d0.loss_bbox: 0.8477, d1.loss_cls: 0.1065, d1.loss_bbox: 0.8110, d2.loss_cls: 0.1195, d2.loss_bbox: 0.7824, d3.loss_cls: 0.1324, d3.loss_bbox: 0.7764, d4.loss_cls: 0.1386, d4.loss_bbox: 0.7729, loss_cls: 0.1412, loss_bbox: 0.7794, loss: 5.5082, grad_norm: 36.8187
2025-11-19 12:53:35,134 - mmdet - INFO - Iter [4150/7752]	lr: 1.001e-04, eta: 1:29:17, time: 1.261, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0996, d0.loss_bbox: 0.8416, d1.loss_cls: 0.1075, d1.loss_bbox: 0.8170, d2.loss_cls: 0.1199, d2.loss_bbox: 0.8048, d3.loss_cls: 0.1328, d3.loss_bbox: 0.8105, d4.loss_cls: 0.1404, d4.loss_bbox: 0.7998, loss_cls: 0.1433, loss_bbox: 0.7966, loss: 5.6138, grad_norm: 36.0471
2025-11-19 12:54:36,650 - mmdet - INFO - Iter [4199/7752]	lr: 1.001e-04, eta: 1:27:55, time: 1.256, data_time: 0.047, memory: 29768, d0.loss_cls: 0.0959, d0.loss_bbox: 0.8476, d1.loss_cls: 0.1056, d1.loss_bbox: 0.8112, d2.loss_cls: 0.1182, d2.loss_bbox: 0.7842, d3.loss_cls: 0.1282, d3.loss_bbox: 0.7859, d4.loss_cls: 0.1349, d4.loss_bbox: 0.7829, loss_cls: 0.1358, loss_bbox: 0.7848, loss: 5.5153, grad_norm: 39.4016
2025-11-19 12:54:36,651 - mmdet - INFO - Saving checkpoint at 13 epochs
2025-11-19 12:54:44,744 - mmdet - INFO - Iter [4200/7752]	lr: 8.706e-05, eta: 1:32:41, time: 6.821, data_time: 5.542, memory: 29768, d0.loss_cls: 0.2092, d0.loss_bbox: 0.9709, d1.loss_cls: 0.2219, d1.loss_bbox: 0.8766, d2.loss_cls: 0.2357, d2.loss_bbox: 0.8478, d3.loss_cls: 0.2475, d3.loss_bbox: 0.8456, d4.loss_cls: 0.2654, d4.loss_bbox: 0.8454, loss_cls: 0.2658, loss_bbox: 0.8304, loss: 6.6622, grad_norm: 21.8409
2025-11-19 12:55:47,269 - mmdet - INFO - Iter [4250/7752]	lr: 8.706e-05, eta: 1:31:10, time: 1.250, data_time: 0.045, memory: 29768, d0.loss_cls: 0.0833, d0.loss_bbox: 0.8342, d1.loss_cls: 0.0901, d1.loss_bbox: 0.8121, d2.loss_cls: 0.1013, d2.loss_bbox: 0.7868, d3.loss_cls: 0.1136, d3.loss_bbox: 0.7774, d4.loss_cls: 0.1192, d4.loss_bbox: 0.7817, loss_cls: 0.1228, loss_bbox: 0.7753, loss: 5.3977, grad_norm: 38.6243
2025-11-19 12:56:50,102 - mmdet - INFO - Iter [4300/7752]	lr: 8.706e-05, eta: 1:29:39, time: 1.257, data_time: 0.046, memory: 29768, d0.loss_cls: 0.0986, d0.loss_bbox: 0.8195, d1.loss_cls: 0.1045, d1.loss_bbox: 0.7920, d2.loss_cls: 0.1143, d2.loss_bbox: 0.7739, d3.loss_cls: 0.1290, d3.loss_bbox: 0.7623, d4.loss_cls: 0.1355, d4.loss_bbox: 0.7613, loss_cls: 0.1391, loss_bbox: 0.7582, loss: 5.3880, grad_norm: 35.1177
2025-11-19 12:57:52,889 - mmdet - INFO - Iter [4350/7752]	lr: 8.706e-05, eta: 1:28:10, time: 1.256, data_time: 0.046, memory: 29768, d0.loss_cls: 0.0835, d0.loss_bbox: 0.7800, d1.loss_cls: 0.0879, d1.loss_bbox: 0.7437, d2.loss_cls: 0.0979, d2.loss_bbox: 0.7198, d3.loss_cls: 0.1149, d3.loss_bbox: 0.7121, d4.loss_cls: 0.1223, d4.loss_bbox: 0.7154, loss_cls: 0.1261, loss_bbox: 0.7159, loss: 5.0195, grad_norm: 37.2362
2025-11-19 12:58:55,626 - mmdet - INFO - Iter [4400/7752]	lr: 8.706e-05, eta: 1:26:40, time: 1.255, data_time: 0.047, memory: 29768, d0.loss_cls: 0.1050, d0.loss_bbox: 0.8958, d1.loss_cls: 0.1115, d1.loss_bbox: 0.8475, d2.loss_cls: 0.1204, d2.loss_bbox: 0.8239, d3.loss_cls: 0.1329, d3.loss_bbox: 0.8216, d4.loss_cls: 0.1376, d4.loss_bbox: 0.8239, loss_cls: 0.1408, loss_bbox: 0.8192, loss: 5.7802, grad_norm: 38.2732
2025-11-19 12:59:58,222 - mmdet - INFO - Iter [4450/7752]	lr: 8.706e-05, eta: 1:25:12, time: 1.252, data_time: 0.047, memory: 29768, d0.loss_cls: 0.1030, d0.loss_bbox: 0.8674, d1.loss_cls: 0.1111, d1.loss_bbox: 0.8208, d2.loss_cls: 0.1207, d2.loss_bbox: 0.8009, d3.loss_cls: 0.1344, d3.loss_bbox: 0.7953, d4.loss_cls: 0.1416, d4.loss_bbox: 0.7850, loss_cls: 0.1442, loss_bbox: 0.7818, loss: 5.6062, grad_norm: 34.5739
2025-11-19 13:01:01,413 - mmdet - INFO - Iter [4500/7752]	lr: 8.706e-05, eta: 1:23:44, time: 1.264, data_time: 0.050, memory: 29768, d0.loss_cls: 0.1004, d0.loss_bbox: 0.7615, d1.loss_cls: 0.1058, d1.loss_bbox: 0.7410, d2.loss_cls: 0.1170, d2.loss_bbox: 0.7250, d3.loss_cls: 0.1364, d3.loss_bbox: 0.7128, d4.loss_cls: 0.1471, d4.loss_bbox: 0.7141, loss_cls: 0.1522, loss_bbox: 0.7061, loss: 5.1194, grad_norm: 32.7441
2025-11-19 13:01:28,923 - mmdet - INFO - Iter [4522/7752]	lr: 8.706e-05, eta: 1:23:31, time: 1.256, data_time: 0.047, memory: 29768, d0.loss_cls: 0.0906, d0.loss_bbox: 0.7778, d1.loss_cls: 0.0967, d1.loss_bbox: 0.7595, d2.loss_cls: 0.1072, d2.loss_bbox: 0.7397, d3.loss_cls: 0.1245, d3.loss_bbox: 0.7305, d4.loss_cls: 0.1351, d4.loss_bbox: 0.7284, loss_cls: 0.1392, loss_bbox: 0.7206, loss: 5.1500, grad_norm: 34.7016
2025-11-19 13:01:28,924 - mmdet - INFO - Saving checkpoint at 14 epochs
2025-11-19 13:02:10,918 - mmdet - INFO - Iter [4550/7752]	lr: 7.424e-05, eta: 1:23:08, time: 1.454, data_time: 0.235, memory: 29768, d0.loss_cls: 0.1070, d0.loss_bbox: 0.8213, d1.loss_cls: 0.1133, d1.loss_bbox: 0.7848, d2.loss_cls: 0.1229, d2.loss_bbox: 0.7666, d3.loss_cls: 0.1396, d3.loss_bbox: 0.7758, d4.loss_cls: 0.1481, d4.loss_bbox: 0.7826, loss_cls: 0.1528, loss_bbox: 0.7777, loss: 5.4926, grad_norm: 33.0011
2025-11-19 13:03:13,969 - mmdet - INFO - Iter [4600/7752]	lr: 7.424e-05, eta: 1:21:40, time: 1.261, data_time: 0.049, memory: 29768, d0.loss_cls: 0.0948, d0.loss_bbox: 0.7802, d1.loss_cls: 0.0978, d1.loss_bbox: 0.7504, d2.loss_cls: 0.1075, d2.loss_bbox: 0.7358, d3.loss_cls: 0.1234, d3.loss_bbox: 0.7316, d4.loss_cls: 0.1341, d4.loss_bbox: 0.7367, loss_cls: 0.1390, loss_bbox: 0.7330, loss: 5.1643, grad_norm: 32.9451
2025-11-19 13:04:16,870 - mmdet - INFO - Iter [4650/7752]	lr: 7.424e-05, eta: 1:20:12, time: 1.258, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0931, d0.loss_bbox: 0.8291, d1.loss_cls: 0.0981, d1.loss_bbox: 0.7969, d2.loss_cls: 0.1062, d2.loss_bbox: 0.7865, d3.loss_cls: 0.1203, d3.loss_bbox: 0.7679, d4.loss_cls: 0.1294, d4.loss_bbox: 0.7631, loss_cls: 0.1325, loss_bbox: 0.7591, loss: 5.3822, grad_norm: 35.7715
2025-11-19 13:05:19,793 - mmdet - INFO - Iter [4700/7752]	lr: 7.424e-05, eta: 1:18:45, time: 1.258, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0861, d0.loss_bbox: 0.7815, d1.loss_cls: 0.0905, d1.loss_bbox: 0.7545, d2.loss_cls: 0.0988, d2.loss_bbox: 0.7384, d3.loss_cls: 0.1127, d3.loss_bbox: 0.7410, d4.loss_cls: 0.1249, d4.loss_bbox: 0.7327, loss_cls: 0.1292, loss_bbox: 0.7268, loss: 5.1171, grad_norm: 37.4694
2025-11-19 13:06:22,810 - mmdet - INFO - Iter [4750/7752]	lr: 7.424e-05, eta: 1:17:18, time: 1.260, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0877, d0.loss_bbox: 0.8070, d1.loss_cls: 0.0922, d1.loss_bbox: 0.7778, d2.loss_cls: 0.0976, d2.loss_bbox: 0.7681, d3.loss_cls: 0.1118, d3.loss_bbox: 0.7529, d4.loss_cls: 0.1229, d4.loss_bbox: 0.7585, loss_cls: 0.1274, loss_bbox: 0.7447, loss: 5.2485, grad_norm: 37.1689
2025-11-19 13:07:25,840 - mmdet - INFO - Iter [4800/7752]	lr: 7.424e-05, eta: 1:15:52, time: 1.261, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0916, d0.loss_bbox: 0.7767, d1.loss_cls: 0.0981, d1.loss_bbox: 0.7414, d2.loss_cls: 0.1064, d2.loss_bbox: 0.7316, d3.loss_cls: 0.1212, d3.loss_bbox: 0.7249, d4.loss_cls: 0.1340, d4.loss_bbox: 0.7268, loss_cls: 0.1387, loss_bbox: 0.7277, loss: 5.1191, grad_norm: 35.9822
2025-11-19 13:08:22,222 - mmdet - INFO - Iter [4845/7752]	lr: 7.424e-05, eta: 1:14:39, time: 1.255, data_time: 0.047, memory: 29768, d0.loss_cls: 0.0839, d0.loss_bbox: 0.7976, d1.loss_cls: 0.0899, d1.loss_bbox: 0.7669, d2.loss_cls: 0.0951, d2.loss_bbox: 0.7669, d3.loss_cls: 0.1077, d3.loss_bbox: 0.7556, d4.loss_cls: 0.1195, d4.loss_bbox: 0.7563, loss_cls: 0.1241, loss_bbox: 0.7497, loss: 5.2133, grad_norm: 38.4195
2025-11-19 13:08:22,222 - mmdet - INFO - Saving checkpoint at 15 epochs
2025-11-19 13:08:35,906 - mmdet - INFO - Iter [4850/7752]	lr: 6.187e-05, eta: 1:15:41, time: 2.482, data_time: 1.250, memory: 29768, d0.loss_cls: 0.1379, d0.loss_bbox: 0.8152, d1.loss_cls: 0.1534, d1.loss_bbox: 0.7952, d2.loss_cls: 0.1630, d2.loss_bbox: 0.7725, d3.loss_cls: 0.1828, d3.loss_bbox: 0.7663, d4.loss_cls: 0.1862, d4.loss_bbox: 0.7575, loss_cls: 0.1935, loss_bbox: 0.7597, loss: 5.6832, grad_norm: 38.4948
2025-11-19 13:09:38,884 - mmdet - INFO - Iter [4900/7752]	lr: 6.187e-05, eta: 1:14:14, time: 1.260, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0855, d0.loss_bbox: 0.7556, d1.loss_cls: 0.0928, d1.loss_bbox: 0.7077, d2.loss_cls: 0.0980, d2.loss_bbox: 0.7024, d3.loss_cls: 0.1106, d3.loss_bbox: 0.6939, d4.loss_cls: 0.1264, d4.loss_bbox: 0.6832, loss_cls: 0.1306, loss_bbox: 0.6798, loss: 4.8666, grad_norm: 37.9415
2025-11-19 13:10:41,788 - mmdet - INFO - Iter [4950/7752]	lr: 6.187e-05, eta: 1:12:47, time: 1.258, data_time: 0.048, memory: 29768, d0.loss_cls: 0.0974, d0.loss_bbox: 0.7589, d1.loss_cls: 0.1020, d1.loss_bbox: 0.7217, d2.loss_cls: 0.1061, d2.loss_bbox: 0.7117, d3.loss_cls: 0.1202, d3.loss_bbox: 0.7070, d4.loss_cls: 0.1346, d4.loss_bbox: 0.7053, loss_cls: 0.1421, loss_bbox: 0.6982, loss: 5.0050, grad_norm: 33.8266
2025-11-19 13:11:44,545 - mmdet - INFO - Iter [5000/7752]	lr: 6.187e-05, eta: 1:11:21, time: 1.255, data_time: 0.046, memory: 29768, d0.loss_cls: 0.0837, d0.loss_bbox: 0.7630, d1.loss_cls: 0.0868, d1.loss_bbox: 0.7414, d2.loss_cls: 0.0934, d2.loss_bbox: 0.7310, d3.loss_cls: 0.1063, d3.loss_bbox: 0.7299, d4.loss_cls: 0.1226, d4.loss_bbox: 0.7223, loss_cls: 0.1305, loss_bbox: 0.7104, loss: 5.0213, grad_norm: 35.6510
2025-11-19 13:12:47,177 - mmdet - INFO - Iter [5050/7752]	lr: 6.187e-05, eta: 1:09:55, time: 1.253, data_time: 0.046, memory: 29768, d0.loss_cls: 0.0798, d0.loss_bbox: 0.7558, d1.loss_cls: 0.0848, d1.loss_bbox: 0.7308, d2.loss_cls: 0.0889, d2.loss_bbox: 0.7248, d3.loss_cls: 0.1010, d3.loss_bbox: 0.7089, d4.loss_cls: 0.1144, d4.loss_bbox: 0.7096, loss_cls: 0.1201, loss_bbox: 0.7008, loss: 4.9199, grad_norm: 37.8085
2025-11-19 13:13:50,102 - mmdet - INFO - Iter [5100/7752]	lr: 6.187e-05, eta: 1:08:30, time: 1.258, data_time: 0.046, memory: 29768, d0.loss_cls: 0.0833, d0.loss_bbox: 0.8341, d1.loss_cls: 0.0866, d1.loss_bbox: 0.7863, d2.loss_cls: 0.0922, d2.loss_bbox: 0.7709, d3.loss_cls: 0.1035, d3.loss_bbox: 0.7565, d4.loss_cls: 0.1138, d4.loss_bbox: 0.7638, loss_cls: 0.1174, loss_bbox: 0.7544, loss: 5.2627, grad_norm: 39.7856
2025-11-19 13:14:52,956 - mmdet - INFO - Iter [5150/7752]	lr: 6.187e-05, eta: 1:07:05, time: 1.257, data_time: 0.047, memory: 29768, d0.loss_cls: 0.0824, d0.loss_bbox: 0.7284, d1.loss_cls: 0.0873, d1.loss_bbox: 0.6976, d2.loss_cls: 0.0935, d2.loss_bbox: 0.6864, d3.loss_cls: 0.1049, d3.loss_bbox: 0.6814, d4.loss_cls: 0.1199, d4.loss_bbox: 0.6800, loss_cls: 0.1270, loss_bbox: 0.6748, loss: 4.7635, grad_norm: 35.1512
2025-11-19 13:15:15,448 - mmdet - INFO - Iter [5168/7752]	lr: 6.187e-05, eta: 1:06:54, time: 1.254, data_time: 0.044, memory: 29768, d0.loss_cls: 0.0793, d0.loss_bbox: 0.7154, d1.loss_cls: 0.0843, d1.loss_bbox: 0.6840, d2.loss_cls: 0.0903, d2.loss_bbox: 0.6681, d3.loss_cls: 0.1026, d3.loss_bbox: 0.6624, d4.loss_cls: 0.1185, d4.loss_bbox: 0.6606, loss_cls: 0.1267, loss_bbox: 0.6584, loss: 4.6507, grad_norm: 34.7014
2025-11-19 13:15:15,449 - mmdet - INFO - Saving checkpoint at 16 epochs
2025-11-19 13:16:02,353 - mmdet - INFO - Iter [5200/7752]	lr: 5.015e-05, eta: 1:06:15, time: 1.426, data_time: 0.216, memory: 29768, d0.loss_cls: 0.0753, d0.loss_bbox: 0.7390, d1.loss_cls: 0.0802, d1.loss_bbox: 0.7062, d2.loss_cls: 0.0857, d2.loss_bbox: 0.6904, d3.loss_cls: 0.0957, d3.loss_bbox: 0.6960, d4.loss_cls: 0.1110, d4.loss_bbox: 0.6955, loss_cls: 0.1182, loss_bbox: 0.6974, loss: 4.7907, grad_norm: 37.0155
2025-11-19 13:17:05,083 - mmdet - INFO - Iter [5250/7752]	lr: 5.015e-05, eta: 1:04:50, time: 1.255, data_time: 0.045, memory: 29768, d0.loss_cls: 0.0787, d0.loss_bbox: 0.7501, d1.loss_cls: 0.0811, d1.loss_bbox: 0.7147, d2.loss_cls: 0.0852, d2.loss_bbox: 0.7097, d3.loss_cls: 0.0964, d3.loss_bbox: 0.6967, d4.loss_cls: 0.1093, d4.loss_bbox: 0.6992, loss_cls: 0.1148, loss_bbox: 0.6949, loss: 4.8310, grad_norm: 38.2740
2025-11-19 13:18:07,668 - mmdet - INFO - Iter [5300/7752]	lr: 5.015e-05, eta: 1:03:25, time: 1.252, data_time: 0.044, memory: 29768, d0.loss_cls: 0.0869, d0.loss_bbox: 0.7004, d1.loss_cls: 0.0901, d1.loss_bbox: 0.6753, d2.loss_cls: 0.0951, d2.loss_bbox: 0.6662, d3.loss_cls: 0.1067, d3.loss_bbox: 0.6656, d4.loss_cls: 0.1229, d4.loss_bbox: 0.6683, loss_cls: 0.1320, loss_bbox: 0.6664, loss: 4.6759, grad_norm: 34.9918
2025-11-19 13:19:10,247 - mmdet - INFO - Iter [5350/7752]	lr: 5.015e-05, eta: 1:02:01, time: 1.252, data_time: 0.045, memory: 29768, d0.loss_cls: 0.0798, d0.loss_bbox: 0.7326, d1.loss_cls: 0.0845, d1.loss_bbox: 0.7006, d2.loss_cls: 0.0904, d2.loss_bbox: 0.6886, d3.loss_cls: 0.1030, d3.loss_bbox: 0.6741, d4.loss_cls: 0.1175, d4.loss_bbox: 0.6745, loss_cls: 0.1273, loss_bbox: 0.6675, loss: 4.7404, grad_norm: 34.2972
2025-11-19 13:20:12,690 - mmdet - INFO - Iter [5400/7752]	lr: 5.015e-05, eta: 1:00:37, time: 1.249, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0829, d0.loss_bbox: 0.7471, d1.loss_cls: 0.0829, d1.loss_bbox: 0.7221, d2.loss_cls: 0.0875, d2.loss_bbox: 0.7151, d3.loss_cls: 0.0988, d3.loss_bbox: 0.7070, d4.loss_cls: 0.1129, d4.loss_bbox: 0.7064, loss_cls: 0.1230, loss_bbox: 0.6940, loss: 4.8797, grad_norm: 37.4291
2025-11-19 13:21:15,308 - mmdet - INFO - Iter [5450/7752]	lr: 5.015e-05, eta: 0:59:13, time: 1.252, data_time: 0.044, memory: 29768, d0.loss_cls: 0.0874, d0.loss_bbox: 0.7259, d1.loss_cls: 0.0925, d1.loss_bbox: 0.6969, d2.loss_cls: 0.0976, d2.loss_bbox: 0.6847, d3.loss_cls: 0.1063, d3.loss_bbox: 0.6797, d4.loss_cls: 0.1203, d4.loss_bbox: 0.6833, loss_cls: 0.1325, loss_bbox: 0.6773, loss: 4.7845, grad_norm: 35.9641
2025-11-19 13:22:06,600 - mmdet - INFO - Iter [5491/7752]	lr: 5.015e-05, eta: 0:58:10, time: 1.252, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0846, d0.loss_bbox: 0.8077, d1.loss_cls: 0.0890, d1.loss_bbox: 0.7708, d2.loss_cls: 0.0938, d2.loss_bbox: 0.7621, d3.loss_cls: 0.1008, d3.loss_bbox: 0.7544, d4.loss_cls: 0.1144, d4.loss_bbox: 0.7436, loss_cls: 0.1240, loss_bbox: 0.7349, loss: 5.1802, grad_norm: 37.2508
2025-11-19 13:22:06,601 - mmdet - INFO - Saving checkpoint at 17 epochs
2025-11-19 13:22:24,927 - mmdet - INFO - Iter [5500/7752]	lr: 3.928e-05, eta: 0:58:29, time: 1.892, data_time: 0.684, memory: 29768, d0.loss_cls: 0.0980, d0.loss_bbox: 0.7765, d1.loss_cls: 0.1002, d1.loss_bbox: 0.7625, d2.loss_cls: 0.1068, d2.loss_bbox: 0.7484, d3.loss_cls: 0.1135, d3.loss_bbox: 0.7479, d4.loss_cls: 0.1293, d4.loss_bbox: 0.7397, loss_cls: 0.1393, loss_bbox: 0.7344, loss: 5.1965, grad_norm: 33.7184
2025-11-19 13:23:27,471 - mmdet - INFO - Iter [5550/7752]	lr: 3.928e-05, eta: 0:57:05, time: 1.251, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0826, d0.loss_bbox: 0.7037, d1.loss_cls: 0.0868, d1.loss_bbox: 0.6793, d2.loss_cls: 0.0916, d2.loss_bbox: 0.6730, d3.loss_cls: 0.1008, d3.loss_bbox: 0.6717, d4.loss_cls: 0.1137, d4.loss_bbox: 0.6748, loss_cls: 0.1258, loss_bbox: 0.6621, loss: 4.6658, grad_norm: 35.1681
2025-11-19 13:24:29,927 - mmdet - INFO - Iter [5600/7752]	lr: 3.928e-05, eta: 0:55:41, time: 1.249, data_time: 0.041, memory: 29768, d0.loss_cls: 0.0764, d0.loss_bbox: 0.7527, d1.loss_cls: 0.0790, d1.loss_bbox: 0.7264, d2.loss_cls: 0.0818, d2.loss_bbox: 0.7197, d3.loss_cls: 0.0897, d3.loss_bbox: 0.7121, d4.loss_cls: 0.1039, d4.loss_bbox: 0.7088, loss_cls: 0.1149, loss_bbox: 0.6880, loss: 4.8534, grad_norm: 36.0233
2025-11-19 13:25:32,379 - mmdet - INFO - Iter [5650/7752]	lr: 3.928e-05, eta: 0:54:18, time: 1.249, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0776, d0.loss_bbox: 0.7454, d1.loss_cls: 0.0808, d1.loss_bbox: 0.7142, d2.loss_cls: 0.0863, d2.loss_bbox: 0.6934, d3.loss_cls: 0.0949, d3.loss_bbox: 0.6835, d4.loss_cls: 0.1072, d4.loss_bbox: 0.6702, loss_cls: 0.1190, loss_bbox: 0.6660, loss: 4.7384, grad_norm: 37.7933
2025-11-19 13:26:34,997 - mmdet - INFO - Iter [5700/7752]	lr: 3.928e-05, eta: 0:52:55, time: 1.252, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0780, d0.loss_bbox: 0.7158, d1.loss_cls: 0.0803, d1.loss_bbox: 0.6816, d2.loss_cls: 0.0822, d2.loss_bbox: 0.6775, d3.loss_cls: 0.0889, d3.loss_bbox: 0.6833, d4.loss_cls: 0.1023, d4.loss_bbox: 0.6852, loss_cls: 0.1173, loss_bbox: 0.6748, loss: 4.6672, grad_norm: 34.3460
2025-11-19 13:27:37,658 - mmdet - INFO - Iter [5750/7752]	lr: 3.928e-05, eta: 0:51:33, time: 1.253, data_time: 0.044, memory: 29768, d0.loss_cls: 0.0803, d0.loss_bbox: 0.6952, d1.loss_cls: 0.0823, d1.loss_bbox: 0.6713, d2.loss_cls: 0.0874, d2.loss_bbox: 0.6601, d3.loss_cls: 0.0950, d3.loss_bbox: 0.6554, d4.loss_cls: 0.1081, d4.loss_bbox: 0.6530, loss_cls: 0.1224, loss_bbox: 0.6409, loss: 4.5514, grad_norm: 34.2960
2025-11-19 13:28:40,384 - mmdet - INFO - Iter [5800/7752]	lr: 3.928e-05, eta: 0:50:11, time: 1.255, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0758, d0.loss_bbox: 0.7070, d1.loss_cls: 0.0789, d1.loss_bbox: 0.6714, d2.loss_cls: 0.0810, d2.loss_bbox: 0.6609, d3.loss_cls: 0.0873, d3.loss_bbox: 0.6621, d4.loss_cls: 0.0994, d4.loss_bbox: 0.6625, loss_cls: 0.1125, loss_bbox: 0.6424, loss: 4.5412, grad_norm: 37.6840
2025-11-19 13:28:57,868 - mmdet - INFO - Iter [5814/7752]	lr: 3.928e-05, eta: 0:50:03, time: 1.252, data_time: 0.041, memory: 29768, d0.loss_cls: 0.0824, d0.loss_bbox: 0.7227, d1.loss_cls: 0.0866, d1.loss_bbox: 0.6834, d2.loss_cls: 0.0886, d2.loss_bbox: 0.6743, d3.loss_cls: 0.0956, d3.loss_bbox: 0.6755, d4.loss_cls: 0.1076, d4.loss_bbox: 0.6719, loss_cls: 0.1207, loss_bbox: 0.6507, loss: 4.6600, grad_norm: 36.3351
2025-11-19 13:28:57,869 - mmdet - INFO - Saving checkpoint at 18 epochs
2025-11-19 13:29:49,884 - mmdet - INFO - Iter [5850/7752]	lr: 2.946e-05, eta: 0:49:12, time: 1.411, data_time: 0.206, memory: 29768, d0.loss_cls: 0.0761, d0.loss_bbox: 0.6737, d1.loss_cls: 0.0796, d1.loss_bbox: 0.6432, d2.loss_cls: 0.0825, d2.loss_bbox: 0.6495, d3.loss_cls: 0.0902, d3.loss_bbox: 0.6413, d4.loss_cls: 0.1021, d4.loss_bbox: 0.6354, loss_cls: 0.1147, loss_bbox: 0.6264, loss: 4.4147, grad_norm: 34.0692
2025-11-19 13:30:52,415 - mmdet - INFO - Iter [5900/7752]	lr: 2.946e-05, eta: 0:47:49, time: 1.251, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0687, d0.loss_bbox: 0.7144, d1.loss_cls: 0.0699, d1.loss_bbox: 0.6718, d2.loss_cls: 0.0725, d2.loss_bbox: 0.6686, d3.loss_cls: 0.0789, d3.loss_bbox: 0.6539, d4.loss_cls: 0.0901, d4.loss_bbox: 0.6554, loss_cls: 0.1010, loss_bbox: 0.6455, loss: 4.4907, grad_norm: 38.1210
2025-11-19 13:31:54,956 - mmdet - INFO - Iter [5950/7752]	lr: 2.946e-05, eta: 0:46:27, time: 1.251, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0807, d0.loss_bbox: 0.7204, d1.loss_cls: 0.0835, d1.loss_bbox: 0.6893, d2.loss_cls: 0.0877, d2.loss_bbox: 0.6736, d3.loss_cls: 0.0944, d3.loss_bbox: 0.6715, d4.loss_cls: 0.1061, d4.loss_bbox: 0.6703, loss_cls: 0.1195, loss_bbox: 0.6634, loss: 4.6603, grad_norm: 36.8493
2025-11-19 13:32:57,515 - mmdet - INFO - Iter [6000/7752]	lr: 2.946e-05, eta: 0:45:06, time: 1.251, data_time: 0.044, memory: 29768, d0.loss_cls: 0.0840, d0.loss_bbox: 0.7250, d1.loss_cls: 0.0860, d1.loss_bbox: 0.6936, d2.loss_cls: 0.0895, d2.loss_bbox: 0.6877, d3.loss_cls: 0.0927, d3.loss_bbox: 0.6883, d4.loss_cls: 0.1051, d4.loss_bbox: 0.6839, loss_cls: 0.1204, loss_bbox: 0.6736, loss: 4.7298, grad_norm: 35.9977
2025-11-19 13:34:00,294 - mmdet - INFO - Iter [6050/7752]	lr: 2.946e-05, eta: 0:43:44, time: 1.256, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0753, d0.loss_bbox: 0.6803, d1.loss_cls: 0.0769, d1.loss_bbox: 0.6506, d2.loss_cls: 0.0799, d2.loss_bbox: 0.6470, d3.loss_cls: 0.0845, d3.loss_bbox: 0.6419, d4.loss_cls: 0.0943, d4.loss_bbox: 0.6418, loss_cls: 0.1073, loss_bbox: 0.6289, loss: 4.4087, grad_norm: 36.0004
2025-11-19 13:35:03,042 - mmdet - INFO - Iter [6100/7752]	lr: 2.946e-05, eta: 0:42:23, time: 1.255, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0743, d0.loss_bbox: 0.7009, d1.loss_cls: 0.0763, d1.loss_bbox: 0.6694, d2.loss_cls: 0.0798, d2.loss_bbox: 0.6677, d3.loss_cls: 0.0852, d3.loss_bbox: 0.6597, d4.loss_cls: 0.0960, d4.loss_bbox: 0.6537, loss_cls: 0.1099, loss_bbox: 0.6410, loss: 4.5141, grad_norm: 36.1657
2025-11-19 13:35:49,340 - mmdet - INFO - Iter [6137/7752]	lr: 2.946e-05, eta: 0:41:28, time: 1.251, data_time: 0.041, memory: 29768, d0.loss_cls: 0.0806, d0.loss_bbox: 0.6996, d1.loss_cls: 0.0820, d1.loss_bbox: 0.6698, d2.loss_cls: 0.0862, d2.loss_bbox: 0.6631, d3.loss_cls: 0.0918, d3.loss_bbox: 0.6657, d4.loss_cls: 0.1029, d4.loss_bbox: 0.6653, loss_cls: 0.1169, loss_bbox: 0.6503, loss: 4.5742, grad_norm: 34.3900
2025-11-19 13:35:49,341 - mmdet - INFO - Saving checkpoint at 19 epochs
2025-11-19 13:36:13,011 - mmdet - INFO - Iter [6150/7752]	lr: 2.084e-05, eta: 0:41:25, time: 1.726, data_time: 0.513, memory: 29768, d0.loss_cls: 0.0857, d0.loss_bbox: 0.7102, d1.loss_cls: 0.0861, d1.loss_bbox: 0.6745, d2.loss_cls: 0.0891, d2.loss_bbox: 0.6645, d3.loss_cls: 0.0914, d3.loss_bbox: 0.6611, d4.loss_cls: 0.1027, d4.loss_bbox: 0.6556, loss_cls: 0.1148, loss_bbox: 0.6467, loss: 4.5824, grad_norm: 35.6870
2025-11-19 13:37:15,819 - mmdet - INFO - Iter [6200/7752]	lr: 2.084e-05, eta: 0:40:04, time: 1.256, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0693, d0.loss_bbox: 0.6525, d1.loss_cls: 0.0713, d1.loss_bbox: 0.6195, d2.loss_cls: 0.0750, d2.loss_bbox: 0.6052, d3.loss_cls: 0.0801, d3.loss_bbox: 0.6093, d4.loss_cls: 0.0905, d4.loss_bbox: 0.6120, loss_cls: 0.1037, loss_bbox: 0.6036, loss: 4.1919, grad_norm: 35.1433
2025-11-19 13:38:18,406 - mmdet - INFO - Iter [6250/7752]	lr: 2.084e-05, eta: 0:38:43, time: 1.252, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0741, d0.loss_bbox: 0.6905, d1.loss_cls: 0.0760, d1.loss_bbox: 0.6687, d2.loss_cls: 0.0784, d2.loss_bbox: 0.6614, d3.loss_cls: 0.0841, d3.loss_bbox: 0.6661, d4.loss_cls: 0.0944, d4.loss_bbox: 0.6580, loss_cls: 0.1082, loss_bbox: 0.6462, loss: 4.5061, grad_norm: 34.7013
2025-11-19 13:39:21,111 - mmdet - INFO - Iter [6300/7752]	lr: 2.084e-05, eta: 0:37:22, time: 1.254, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0728, d0.loss_bbox: 0.7257, d1.loss_cls: 0.0767, d1.loss_bbox: 0.6751, d2.loss_cls: 0.0801, d2.loss_bbox: 0.6642, d3.loss_cls: 0.0830, d3.loss_bbox: 0.6612, d4.loss_cls: 0.0926, d4.loss_bbox: 0.6594, loss_cls: 0.1063, loss_bbox: 0.6414, loss: 4.5384, grad_norm: 36.7485
2025-11-19 13:40:23,682 - mmdet - INFO - Iter [6350/7752]	lr: 2.084e-05, eta: 0:36:02, time: 1.251, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0820, d0.loss_bbox: 0.7062, d1.loss_cls: 0.0841, d1.loss_bbox: 0.6680, d2.loss_cls: 0.0877, d2.loss_bbox: 0.6561, d3.loss_cls: 0.0936, d3.loss_bbox: 0.6528, d4.loss_cls: 0.1015, d4.loss_bbox: 0.6521, loss_cls: 0.1187, loss_bbox: 0.6277, loss: 4.5304, grad_norm: 34.4064
2025-11-19 13:41:26,321 - mmdet - INFO - Iter [6400/7752]	lr: 2.084e-05, eta: 0:34:41, time: 1.253, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0619, d0.loss_bbox: 0.6797, d1.loss_cls: 0.0634, d1.loss_bbox: 0.6379, d2.loss_cls: 0.0653, d2.loss_bbox: 0.6389, d3.loss_cls: 0.0694, d3.loss_bbox: 0.6413, d4.loss_cls: 0.0789, d4.loss_bbox: 0.6360, loss_cls: 0.0921, loss_bbox: 0.6283, loss: 4.2932, grad_norm: 39.0727
2025-11-19 13:42:28,925 - mmdet - INFO - Iter [6450/7752]	lr: 2.084e-05, eta: 0:33:21, time: 1.252, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0782, d0.loss_bbox: 0.6751, d1.loss_cls: 0.0795, d1.loss_bbox: 0.6403, d2.loss_cls: 0.0821, d2.loss_bbox: 0.6307, d3.loss_cls: 0.0861, d3.loss_bbox: 0.6339, d4.loss_cls: 0.0929, d4.loss_bbox: 0.6341, loss_cls: 0.1076, loss_bbox: 0.6236, loss: 4.3640, grad_norm: 36.1500
2025-11-19 13:42:41,435 - mmdet - INFO - Iter [6460/7752]	lr: 2.084e-05, eta: 0:33:16, time: 1.252, data_time: 0.041, memory: 29768, d0.loss_cls: 0.0812, d0.loss_bbox: 0.6752, d1.loss_cls: 0.0831, d1.loss_bbox: 0.6341, d2.loss_cls: 0.0857, d2.loss_bbox: 0.6249, d3.loss_cls: 0.0899, d3.loss_bbox: 0.6289, d4.loss_cls: 0.0971, d4.loss_bbox: 0.6325, loss_cls: 0.1131, loss_bbox: 0.6200, loss: 4.3658, grad_norm: 34.6685
2025-11-19 13:42:41,435 - mmdet - INFO - Saving checkpoint at 20 epochs
2025-11-19 13:43:38,811 - mmdet - INFO - Iter [6500/7752]	lr: 1.358e-05, eta: 0:32:15, time: 1.403, data_time: 0.190, memory: 29768, d0.loss_cls: 0.0751, d0.loss_bbox: 0.6927, d1.loss_cls: 0.0776, d1.loss_bbox: 0.6589, d2.loss_cls: 0.0814, d2.loss_bbox: 0.6435, d3.loss_cls: 0.0867, d3.loss_bbox: 0.6491, d4.loss_cls: 0.0954, d4.loss_bbox: 0.6352, loss_cls: 0.1109, loss_bbox: 0.6187, loss: 4.4253, grad_norm: 34.2015
2025-11-19 13:44:41,544 - mmdet - INFO - Iter [6550/7752]	lr: 1.358e-05, eta: 0:30:55, time: 1.255, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0649, d0.loss_bbox: 0.6947, d1.loss_cls: 0.0678, d1.loss_bbox: 0.6579, d2.loss_cls: 0.0716, d2.loss_bbox: 0.6474, d3.loss_cls: 0.0739, d3.loss_bbox: 0.6412, d4.loss_cls: 0.0812, d4.loss_bbox: 0.6418, loss_cls: 0.0946, loss_bbox: 0.6286, loss: 4.3655, grad_norm: 38.0417
2025-11-19 13:45:44,425 - mmdet - INFO - Iter [6600/7752]	lr: 1.358e-05, eta: 0:29:36, time: 1.258, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0757, d0.loss_bbox: 0.6585, d1.loss_cls: 0.0790, d1.loss_bbox: 0.6217, d2.loss_cls: 0.0813, d2.loss_bbox: 0.6132, d3.loss_cls: 0.0857, d3.loss_bbox: 0.6132, d4.loss_cls: 0.0935, d4.loss_bbox: 0.6148, loss_cls: 0.1091, loss_bbox: 0.6004, loss: 4.2462, grad_norm: 34.2587
2025-11-19 13:46:47,013 - mmdet - INFO - Iter [6650/7752]	lr: 1.358e-05, eta: 0:28:16, time: 1.252, data_time: 0.044, memory: 29768, d0.loss_cls: 0.0746, d0.loss_bbox: 0.6954, d1.loss_cls: 0.0771, d1.loss_bbox: 0.6528, d2.loss_cls: 0.0804, d2.loss_bbox: 0.6457, d3.loss_cls: 0.0840, d3.loss_bbox: 0.6480, d4.loss_cls: 0.0898, d4.loss_bbox: 0.6510, loss_cls: 0.1047, loss_bbox: 0.6404, loss: 4.4440, grad_norm: 35.9958
2025-11-19 13:47:49,551 - mmdet - INFO - Iter [6700/7752]	lr: 1.358e-05, eta: 0:26:57, time: 1.251, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0668, d0.loss_bbox: 0.6441, d1.loss_cls: 0.0674, d1.loss_bbox: 0.6079, d2.loss_cls: 0.0701, d2.loss_bbox: 0.5970, d3.loss_cls: 0.0741, d3.loss_bbox: 0.5927, d4.loss_cls: 0.0814, d4.loss_bbox: 0.5954, loss_cls: 0.0949, loss_bbox: 0.5809, loss: 4.0728, grad_norm: 37.2681
2025-11-19 13:48:52,079 - mmdet - INFO - Iter [6750/7752]	lr: 1.358e-05, eta: 0:25:38, time: 1.251, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0821, d0.loss_bbox: 0.6852, d1.loss_cls: 0.0847, d1.loss_bbox: 0.6504, d2.loss_cls: 0.0877, d2.loss_bbox: 0.6472, d3.loss_cls: 0.0910, d3.loss_bbox: 0.6482, d4.loss_cls: 0.0991, d4.loss_bbox: 0.6493, loss_cls: 0.1131, loss_bbox: 0.6414, loss: 4.4795, grad_norm: 33.7400
2025-11-19 13:49:33,378 - mmdet - INFO - Iter [6783/7752]	lr: 1.358e-05, eta: 0:24:49, time: 1.251, data_time: 0.040, memory: 29768, d0.loss_cls: 0.0757, d0.loss_bbox: 0.6953, d1.loss_cls: 0.0783, d1.loss_bbox: 0.6598, d2.loss_cls: 0.0806, d2.loss_bbox: 0.6583, d3.loss_cls: 0.0843, d3.loss_bbox: 0.6557, d4.loss_cls: 0.0909, d4.loss_bbox: 0.6593, loss_cls: 0.1043, loss_bbox: 0.6489, loss: 4.4914, grad_norm: 36.7897
2025-11-19 13:49:33,379 - mmdet - INFO - Saving checkpoint at 21 epochs
2025-11-19 13:50:01,719 - mmdet - INFO - Iter [6800/7752]	lr: 7.804e-06, eta: 0:24:30, time: 1.595, data_time: 0.386, memory: 29768, d0.loss_cls: 0.0815, d0.loss_bbox: 0.6884, d1.loss_cls: 0.0822, d1.loss_bbox: 0.6601, d2.loss_cls: 0.0871, d2.loss_bbox: 0.6457, d3.loss_cls: 0.0898, d3.loss_bbox: 0.6399, d4.loss_cls: 0.0963, d4.loss_bbox: 0.6467, loss_cls: 0.1076, loss_bbox: 0.6432, loss: 4.4684, grad_norm: 36.2407
2025-11-19 13:51:04,293 - mmdet - INFO - Iter [6850/7752]	lr: 7.804e-06, eta: 0:23:11, time: 1.251, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0673, d0.loss_bbox: 0.6796, d1.loss_cls: 0.0674, d1.loss_bbox: 0.6352, d2.loss_cls: 0.0706, d2.loss_bbox: 0.6269, d3.loss_cls: 0.0735, d3.loss_bbox: 0.6265, d4.loss_cls: 0.0797, d4.loss_bbox: 0.6281, loss_cls: 0.0932, loss_bbox: 0.6105, loss: 4.2586, grad_norm: 35.3815
2025-11-19 13:52:06,848 - mmdet - INFO - Iter [6900/7752]	lr: 7.804e-06, eta: 0:21:52, time: 1.251, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0689, d0.loss_bbox: 0.6987, d1.loss_cls: 0.0703, d1.loss_bbox: 0.6520, d2.loss_cls: 0.0738, d2.loss_bbox: 0.6378, d3.loss_cls: 0.0768, d3.loss_bbox: 0.6305, d4.loss_cls: 0.0834, d4.loss_bbox: 0.6411, loss_cls: 0.0964, loss_bbox: 0.6314, loss: 4.3611, grad_norm: 37.1791
2025-11-19 13:53:09,314 - mmdet - INFO - Iter [6950/7752]	lr: 7.804e-06, eta: 0:20:34, time: 1.249, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0718, d0.loss_bbox: 0.6860, d1.loss_cls: 0.0728, d1.loss_bbox: 0.6540, d2.loss_cls: 0.0756, d2.loss_bbox: 0.6472, d3.loss_cls: 0.0788, d3.loss_bbox: 0.6486, d4.loss_cls: 0.0839, d4.loss_bbox: 0.6494, loss_cls: 0.0988, loss_bbox: 0.6344, loss: 4.4014, grad_norm: 36.1424
2025-11-19 13:54:11,688 - mmdet - INFO - Iter [7000/7752]	lr: 7.804e-06, eta: 0:19:15, time: 1.247, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0702, d0.loss_bbox: 0.6597, d1.loss_cls: 0.0727, d1.loss_bbox: 0.6307, d2.loss_cls: 0.0748, d2.loss_bbox: 0.6221, d3.loss_cls: 0.0776, d3.loss_bbox: 0.6234, d4.loss_cls: 0.0860, d4.loss_bbox: 0.6219, loss_cls: 0.0987, loss_bbox: 0.6094, loss: 4.2474, grad_norm: 34.4025
2025-11-19 13:55:14,187 - mmdet - INFO - Iter [7050/7752]	lr: 7.804e-06, eta: 0:17:57, time: 1.250, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0692, d0.loss_bbox: 0.6570, d1.loss_cls: 0.0712, d1.loss_bbox: 0.6227, d2.loss_cls: 0.0728, d2.loss_bbox: 0.6201, d3.loss_cls: 0.0759, d3.loss_bbox: 0.6221, d4.loss_cls: 0.0832, d4.loss_bbox: 0.6174, loss_cls: 0.0956, loss_bbox: 0.6044, loss: 4.2117, grad_norm: 35.4300
2025-11-19 13:56:17,031 - mmdet - INFO - Iter [7100/7752]	lr: 7.804e-06, eta: 0:16:39, time: 1.257, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0741, d0.loss_bbox: 0.6527, d1.loss_cls: 0.0755, d1.loss_bbox: 0.6176, d2.loss_cls: 0.0792, d2.loss_bbox: 0.6063, d3.loss_cls: 0.0822, d3.loss_bbox: 0.6091, d4.loss_cls: 0.0897, d4.loss_bbox: 0.6042, loss_cls: 0.1019, loss_bbox: 0.5946, loss: 4.1869, grad_norm: 33.5154
2025-11-19 13:56:24,486 - mmdet - INFO - Iter [7106/7752]	lr: 7.804e-06, eta: 0:16:34, time: 1.256, data_time: 0.041, memory: 29768, d0.loss_cls: 0.0757, d0.loss_bbox: 0.6605, d1.loss_cls: 0.0769, d1.loss_bbox: 0.6261, d2.loss_cls: 0.0807, d2.loss_bbox: 0.6134, d3.loss_cls: 0.0836, d3.loss_bbox: 0.6161, d4.loss_cls: 0.0912, d4.loss_bbox: 0.6097, loss_cls: 0.1035, loss_bbox: 0.6021, loss: 4.2396, grad_norm: 33.2253
2025-11-19 13:56:24,486 - mmdet - INFO - Saving checkpoint at 22 epochs
2025-11-19 13:57:26,799 - mmdet - INFO - Iter [7150/7752]	lr: 3.604e-06, eta: 0:15:27, time: 1.389, data_time: 0.170, memory: 29768, d0.loss_cls: 0.0802, d0.loss_bbox: 0.7389, d1.loss_cls: 0.0823, d1.loss_bbox: 0.7032, d2.loss_cls: 0.0862, d2.loss_bbox: 0.6875, d3.loss_cls: 0.0907, d3.loss_bbox: 0.6831, d4.loss_cls: 0.0972, d4.loss_bbox: 0.6781, loss_cls: 0.1093, loss_bbox: 0.6663, loss: 4.7030, grad_norm: 36.7101
2025-11-19 13:58:29,235 - mmdet - INFO - Iter [7200/7752]	lr: 3.604e-06, eta: 0:14:09, time: 1.249, data_time: 0.040, memory: 29768, d0.loss_cls: 0.0684, d0.loss_bbox: 0.6638, d1.loss_cls: 0.0697, d1.loss_bbox: 0.6257, d2.loss_cls: 0.0723, d2.loss_bbox: 0.6134, d3.loss_cls: 0.0754, d3.loss_bbox: 0.6093, d4.loss_cls: 0.0828, d4.loss_bbox: 0.6053, loss_cls: 0.0949, loss_bbox: 0.5919, loss: 4.1727, grad_norm: 37.1877
2025-11-19 13:59:31,669 - mmdet - INFO - Iter [7250/7752]	lr: 3.604e-06, eta: 0:12:51, time: 1.249, data_time: 0.040, memory: 29768, d0.loss_cls: 0.0673, d0.loss_bbox: 0.6743, d1.loss_cls: 0.0685, d1.loss_bbox: 0.6378, d2.loss_cls: 0.0709, d2.loss_bbox: 0.6304, d3.loss_cls: 0.0737, d3.loss_bbox: 0.6292, d4.loss_cls: 0.0795, d4.loss_bbox: 0.6307, loss_cls: 0.0918, loss_bbox: 0.6203, loss: 4.2743, grad_norm: 35.8707
2025-11-19 14:00:34,129 - mmdet - INFO - Iter [7300/7752]	lr: 3.604e-06, eta: 0:11:33, time: 1.249, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0772, d0.loss_bbox: 0.6404, d1.loss_cls: 0.0789, d1.loss_bbox: 0.6146, d2.loss_cls: 0.0814, d2.loss_bbox: 0.6049, d3.loss_cls: 0.0855, d3.loss_bbox: 0.6061, d4.loss_cls: 0.0925, d4.loss_bbox: 0.6033, loss_cls: 0.1051, loss_bbox: 0.5950, loss: 4.1849, grad_norm: 33.4488
2025-11-19 14:01:36,679 - mmdet - INFO - Iter [7350/7752]	lr: 3.604e-06, eta: 0:10:16, time: 1.251, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0712, d0.loss_bbox: 0.6573, d1.loss_cls: 0.0712, d1.loss_bbox: 0.6198, d2.loss_cls: 0.0736, d2.loss_bbox: 0.6123, d3.loss_cls: 0.0772, d3.loss_bbox: 0.6123, d4.loss_cls: 0.0846, d4.loss_bbox: 0.6070, loss_cls: 0.0965, loss_bbox: 0.5984, loss: 4.1815, grad_norm: 35.5131
2025-11-19 14:02:39,500 - mmdet - INFO - Iter [7400/7752]	lr: 3.604e-06, eta: 0:08:58, time: 1.256, data_time: 0.043, memory: 29768, d0.loss_cls: 0.0598, d0.loss_bbox: 0.6536, d1.loss_cls: 0.0593, d1.loss_bbox: 0.6195, d2.loss_cls: 0.0637, d2.loss_bbox: 0.6073, d3.loss_cls: 0.0657, d3.loss_bbox: 0.6113, d4.loss_cls: 0.0717, d4.loss_bbox: 0.6159, loss_cls: 0.0851, loss_bbox: 0.6044, loss: 4.1171, grad_norm: 36.6760
2025-11-19 14:03:15,799 - mmdet - INFO - Iter [7429/7752]	lr: 3.604e-06, eta: 0:08:15, time: 1.256, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0631, d0.loss_bbox: 0.6431, d1.loss_cls: 0.0631, d1.loss_bbox: 0.6151, d2.loss_cls: 0.0671, d2.loss_bbox: 0.6050, d3.loss_cls: 0.0680, d3.loss_bbox: 0.6116, d4.loss_cls: 0.0762, d4.loss_bbox: 0.6124, loss_cls: 0.0879, loss_bbox: 0.6050, loss: 4.1176, grad_norm: 35.1802
2025-11-19 14:03:15,799 - mmdet - INFO - Saving checkpoint at 23 epochs
2025-11-19 14:03:49,311 - mmdet - INFO - Iter [7450/7752]	lr: 1.055e-06, eta: 0:07:44, time: 1.538, data_time: 0.329, memory: 29768, d0.loss_cls: 0.0673, d0.loss_bbox: 0.6257, d1.loss_cls: 0.0677, d1.loss_bbox: 0.6025, d2.loss_cls: 0.0700, d2.loss_bbox: 0.5937, d3.loss_cls: 0.0740, d3.loss_bbox: 0.5927, d4.loss_cls: 0.0804, d4.loss_bbox: 0.6021, loss_cls: 0.0947, loss_bbox: 0.5926, loss: 4.0635, grad_norm: 30.8466
2025-11-19 14:04:52,089 - mmdet - INFO - Iter [7500/7752]	lr: 1.055e-06, eta: 0:06:27, time: 1.256, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0766, d0.loss_bbox: 0.6884, d1.loss_cls: 0.0782, d1.loss_bbox: 0.6536, d2.loss_cls: 0.0812, d2.loss_bbox: 0.6422, d3.loss_cls: 0.0846, d3.loss_bbox: 0.6454, d4.loss_cls: 0.0911, d4.loss_bbox: 0.6398, loss_cls: 0.1036, loss_bbox: 0.6283, loss: 4.4130, grad_norm: 34.2545
2025-11-19 14:05:54,640 - mmdet - INFO - Iter [7550/7752]	lr: 1.055e-06, eta: 0:05:10, time: 1.251, data_time: 0.040, memory: 29768, d0.loss_cls: 0.0707, d0.loss_bbox: 0.6925, d1.loss_cls: 0.0709, d1.loss_bbox: 0.6595, d2.loss_cls: 0.0740, d2.loss_bbox: 0.6480, d3.loss_cls: 0.0769, d3.loss_bbox: 0.6440, d4.loss_cls: 0.0831, d4.loss_bbox: 0.6406, loss_cls: 0.0957, loss_bbox: 0.6303, loss: 4.3862, grad_norm: 37.7195
2025-11-19 14:06:57,321 - mmdet - INFO - Iter [7600/7752]	lr: 1.055e-06, eta: 0:03:53, time: 1.254, data_time: 0.041, memory: 29768, d0.loss_cls: 0.0705, d0.loss_bbox: 0.6860, d1.loss_cls: 0.0724, d1.loss_bbox: 0.6462, d2.loss_cls: 0.0750, d2.loss_bbox: 0.6378, d3.loss_cls: 0.0783, d3.loss_bbox: 0.6390, d4.loss_cls: 0.0840, d4.loss_bbox: 0.6378, loss_cls: 0.0960, loss_bbox: 0.6317, loss: 4.3547, grad_norm: 37.9064
2025-11-19 14:07:59,736 - mmdet - INFO - Iter [7650/7752]	lr: 1.055e-06, eta: 0:02:36, time: 1.248, data_time: 0.042, memory: 29768, d0.loss_cls: 0.0635, d0.loss_bbox: 0.6111, d1.loss_cls: 0.0640, d1.loss_bbox: 0.5802, d2.loss_cls: 0.0675, d2.loss_bbox: 0.5722, d3.loss_cls: 0.0703, d3.loss_bbox: 0.5720, d4.loss_cls: 0.0772, d4.loss_bbox: 0.5770, loss_cls: 0.0884, loss_bbox: 0.5673, loss: 3.9106, grad_norm: 34.5732
2025-11-19 14:09:02,261 - mmdet - INFO - Iter [7700/7752]	lr: 1.055e-06, eta: 0:01:19, time: 1.250, data_time: 0.040, memory: 29768, d0.loss_cls: 0.0710, d0.loss_bbox: 0.6241, d1.loss_cls: 0.0716, d1.loss_bbox: 0.5963, d2.loss_cls: 0.0755, d2.loss_bbox: 0.5904, d3.loss_cls: 0.0785, d3.loss_bbox: 0.5852, d4.loss_cls: 0.0864, d4.loss_bbox: 0.5873, loss_cls: 0.0989, loss_bbox: 0.5844, loss: 4.0495, grad_norm: 34.8358
2025-11-19 14:10:05,073 - mmdet - INFO - Iter [7750/7752]	lr: 1.055e-06, eta: 0:00:03, time: 1.256, data_time: 0.039, memory: 29768, d0.loss_cls: 0.0713, d0.loss_bbox: 0.6733, d1.loss_cls: 0.0733, d1.loss_bbox: 0.6383, d2.loss_cls: 0.0751, d2.loss_bbox: 0.6311, d3.loss_cls: 0.0783, d3.loss_bbox: 0.6273, d4.loss_cls: 0.0839, d4.loss_bbox: 0.6279, loss_cls: 0.0977, loss_bbox: 0.6141, loss: 4.2916, grad_norm: 35.6311
2025-11-19 14:10:07,615 - mmdet - INFO - Iter [7752/7752]	lr: 1.055e-06, eta: 0:00:00, time: 1.257, data_time: 0.039, memory: 29768, d0.loss_cls: 0.0709, d0.loss_bbox: 0.6736, d1.loss_cls: 0.0730, d1.loss_bbox: 0.6385, d2.loss_cls: 0.0746, d2.loss_bbox: 0.6309, d3.loss_cls: 0.0778, d3.loss_bbox: 0.6298, d4.loss_cls: 0.0835, d4.loss_bbox: 0.6319, loss_cls: 0.0972, loss_bbox: 0.6195, loss: 4.3011, grad_norm: 36.3044
2025-11-19 14:10:07,616 - mmdet - INFO - Saving checkpoint at 24 epochs
